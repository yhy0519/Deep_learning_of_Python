{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ■ 4장 전체코드 디버깅하며 이해하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 2층 신경망 클래스를 만듭니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. 필요한 패키지를 불러옵니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\n",
    "from common.functions import *\n",
    "from common.gradient import numerical_gradient\n",
    "from dataset.mnist import load_mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 가중치 행렬 W1, W2, b1, b2 를 구성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "신경망이 생성되었습니다\n",
      "dict_keys(['W1', 'b1', 'W2', 'b2'])\n",
      "(784, 50)\n",
      "(50,)\n",
      "(50, 10)\n",
      "(10,)\n"
     ]
    }
   ],
   "source": [
    "class TwoLayerNet:\n",
    "## 1. 가중치 행렬 W1, W2, b1, b2 를 구성합니다.\n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):\n",
    "        # 가중치 초기화\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "        print('신경망이 생성되었습니다')\n",
    "\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "print(network.params.keys())  # 가중치 행렬을 가지고 있는 params 딕셔너리의 키 값들 확인\n",
    "print(network.params['W1'].shape)\n",
    "print(network.params['b1'].shape)\n",
    "print(network.params['W2'].shape)\n",
    "print(network.params['b2'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 입력데이터(필기체)를 넣고 1층과 2층을 거쳐서 확률벡터를 출력하는 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "신경망이 생성되었습니다\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(60000, 10)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class TwoLayerNet:\n",
    "## 1. 가중치 행렬 W1, W2, b1, b2 를 구성합니다.\n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):\n",
    "        # 가중치 초기화\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "        print('신경망이 생성되었습니다')\n",
    "        \n",
    "#network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "#print(network.params.keys())  # 가중치 행렬을 가지고 있는 params 딕셔너리의 키 값들 확인\n",
    "                               # dict_keys(['W1', 'b1', 'W2', 'b2'])\n",
    "#print(network.params['W1'].shape)  # (784, 50)\n",
    "#print(network.params['W2'].shape)  # (50, 10)\n",
    "#print(network.params['b1'].shape)  # (50,)\n",
    "#print(network.params['b2'].shape)  # (10,)\n",
    "\n",
    "## 2. 입력데이터(필기체)를 넣고 1층과 2층을 거쳐서 확률벡터를 출력하는 함수\n",
    "    def predict(self, x):\n",
    "        W1, W2 = self.params['W1'], self.params['W2']   # 가중치 불러오는 코드\n",
    "        b1, b2 = self.params['b1'], self.params['b2']   # 편향 불러오는 코드\n",
    "    \n",
    "        a1 = np.dot(x, W1) + b1   # 1층 구성\n",
    "        z1 = sigmoid(a1)          # 1층 시그모이드 함수\n",
    "        a2 = np.dot(z1, W2) + b2  # 2층 구성\n",
    "        y = softmax(a2)           # 2층이 출력층이라 소프트맥스 함수\n",
    "        \n",
    "        return y\n",
    "\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "network.predict(x_train).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 오차(에러) 를 출력하는 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "신경망이 생성되었습니다\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2.3746952304669624"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class TwoLayerNet:\n",
    "## 1. 가중치 행렬 W1, W2, b1, b2 를 구성합니다.\n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):\n",
    "        # 가중치 초기화\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "        print('신경망이 생성되었습니다')\n",
    "\n",
    "#network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "#print(network.params.keys())  # 가중치 행렬을 가지고 있는 params 딕셔너리의 키 값들 확인\n",
    "                               # dict_keys(['W1', 'b1', 'W2', 'b2'])\n",
    "#print(network.params['W1'].shape)  # (784, 50)\n",
    "#print(network.params['W2'].shape)  # (50, 10)\n",
    "#print(network.params['b1'].shape)  # (50,)\n",
    "#print(network.params['b2'].shape)  # (10,)\n",
    "\n",
    "## 2. 입력데이터(필기체)를 넣고 1층과 2층을 거쳐서 확률벡터를 출력하는 함수\n",
    "    def predict(self, x):\n",
    "        W1, W2 = self.params['W1'], self.params['W2']   # 가중치 불러오는 코드\n",
    "        b1, b2 = self.params['b1'], self.params['b2']   # 편향 불러오는 코드\n",
    "    \n",
    "        a1 = np.dot(x, W1) + b1   # 1층 구성\n",
    "        z1 = sigmoid(a1)          # 1층 시그모이드 함수\n",
    "        a2 = np.dot(z1, W2) + b2  # 2층 구성\n",
    "        y = softmax(a2)           # 2층이 출력층이라 소프트맥스 함수\n",
    "        \n",
    "        return y\n",
    "    \n",
    "#network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "#(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "#network.predict(x_train).shape  # (60000, 10)\n",
    "\n",
    "## 3. 오차(에러) 를 출력하는 함수\n",
    "    # x : 입력 데이터, t : 정답 레이블\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        \n",
    "        return cross_entropy_error(y, t)   # 오차의 평균을 출력\n",
    "\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "network.loss(x_train[100], t_train[100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 정확도를 출력하는 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "신경망이 생성되었습니다\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.11"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class TwoLayerNet:\n",
    "## 1. 가중치 행렬 W1, W2, b1, b2 를 구성합니다.\n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):\n",
    "        # 가중치 초기화\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "        print('신경망이 생성되었습니다')\n",
    "\n",
    "#network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "#print(network.params.keys())  # 가중치 행렬을 가지고 있는 params 딕셔너리의 키 값들 확인\n",
    "                               # dict_keys(['W1', 'b1', 'W2', 'b2'])\n",
    "#print(network.params['W1'].shape)  # (784, 50)\n",
    "#print(network.params['W2'].shape)  # (50, 10)\n",
    "#print(network.params['b1'].shape)  # (50,)\n",
    "#print(network.params['b2'].shape)  # (10,)\n",
    "\n",
    "## 2. 입력데이터(필기체)를 넣고 1층과 2층을 거쳐서 확률벡터를 출력하는 함수\n",
    "    def predict(self, x):\n",
    "        W1, W2 = self.params['W1'], self.params['W2']   # 가중치 불러오는 코드\n",
    "        b1, b2 = self.params['b1'], self.params['b2']   # 편향 불러오는 코드\n",
    "    \n",
    "        a1 = np.dot(x, W1) + b1   # 1층 구성\n",
    "        z1 = sigmoid(a1)          # 1층 시그모이드 함수\n",
    "        a2 = np.dot(z1, W2) + b2  # 2층 구성\n",
    "        y = softmax(a2)           # 2층이 출력층이라 소프트맥스 함수\n",
    "        \n",
    "        return y\n",
    "\n",
    "#network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "#(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "#network.predict(x_train).shape  # (60000, 10)\n",
    "\n",
    "## 3. 오차(에러) 를 출력하는 함수\n",
    "    # x : 입력 데이터, t : 정답 레이블\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        \n",
    "        return cross_entropy_error(y, t)   # 오차의 평균을 출력\n",
    "    \n",
    "#network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "#(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "#network.loss(x_train[100], t_train[100])   # 2.2956634380340786\n",
    "\n",
    "\n",
    "## 4. 정확도를 출력하는 함수\n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis=1)\n",
    "        t = np.argmax(t, axis=1)\n",
    "        \n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "        return accuracy\n",
    "\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "network.accuracy(x_train[:100,], t_train[:100,]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. 편미분해서 기울기를 출력하는 함수(4개의 기울기를 출력, W1, b1, W2, b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "신경망이 생성되었습니다\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'W1': array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]]),\n",
       " 'b1': array([ 9.20525745e-05, -3.91378774e-05,  2.08260935e-04,  2.08467121e-04,\n",
       "         1.89224554e-04, -2.13480500e-05, -1.23029924e-04, -3.53145513e-06,\n",
       "        -1.24021806e-04,  2.58713628e-05,  8.86831875e-05, -1.41629433e-04,\n",
       "         3.86928134e-05, -2.14805134e-04, -3.01998841e-04, -2.99614378e-05,\n",
       "        -6.78501899e-05, -1.31373701e-04, -8.91002827e-07, -1.19435140e-04,\n",
       "        -4.60799243e-05,  8.91014551e-05, -6.41049214e-05,  2.42195886e-04,\n",
       "        -1.06015539e-04, -7.59590368e-05,  5.15620702e-05, -1.12656506e-05,\n",
       "        -9.48551016e-06, -1.49661039e-05, -3.12726713e-04, -2.31749626e-04,\n",
       "        -2.04366069e-04, -1.33158895e-04, -5.46299272e-05, -6.79781875e-05,\n",
       "         8.49370174e-05,  1.64406710e-05,  7.16701409e-05, -3.24572480e-04,\n",
       "         2.16566840e-04,  1.36914613e-04, -9.54398782e-07, -1.10928402e-04,\n",
       "         9.30779787e-05,  4.57471527e-05, -1.75530961e-04,  1.82660456e-04,\n",
       "        -5.95886762e-05, -2.78086139e-04]),\n",
       " 'W2': array([[-0.01668829, -0.02021311,  0.0225344 , -0.0077574 , -0.00100708,\n",
       "          0.02109132, -0.00635213,  0.00501685,  0.00791664, -0.00454121],\n",
       "        [-0.01559999, -0.0179983 ,  0.02154138, -0.00690863, -0.00034435,\n",
       "          0.02008757, -0.00415024,  0.00165256,  0.00736116, -0.00564116],\n",
       "        [-0.01565199, -0.01663803,  0.02200708, -0.00693305, -0.00334449,\n",
       "          0.0216297 , -0.0066218 ,  0.00445448,  0.00732167, -0.00622359],\n",
       "        [-0.01679257, -0.01704708,  0.02215735, -0.00721447, -0.00186437,\n",
       "          0.02049882, -0.00597357,  0.00308053,  0.00857051, -0.00541515],\n",
       "        [-0.01633004, -0.01782587,  0.02332919, -0.00680797, -0.00190456,\n",
       "          0.0217553 , -0.00854995,  0.00325877,  0.00940936, -0.00633424],\n",
       "        [-0.01730858, -0.01725165,  0.02079927, -0.00665639, -0.00213256,\n",
       "          0.02135356, -0.00522369,  0.00372167,  0.00787608, -0.00517769],\n",
       "        [-0.01424726, -0.017286  ,  0.02132828, -0.00703224, -0.00257529,\n",
       "          0.01848764, -0.00599123,  0.003312  ,  0.00838581, -0.00438171],\n",
       "        [-0.01403415, -0.02014787,  0.02025338, -0.00546818, -0.00199936,\n",
       "          0.01859382, -0.00510453,  0.00377275,  0.00822534, -0.0040912 ],\n",
       "        [-0.01477129, -0.0179205 ,  0.02159694, -0.0077271 , -0.00308042,\n",
       "          0.02139642, -0.00791748,  0.00266424,  0.01059774, -0.00483855],\n",
       "        [-0.01380347, -0.01706247,  0.02009034, -0.01003467, -0.00266484,\n",
       "          0.02038661, -0.00442048,  0.00398061,  0.00885717, -0.0053288 ],\n",
       "        [-0.01619749, -0.01747833,  0.02189272, -0.00695345, -0.00164084,\n",
       "          0.02044567, -0.00820418,  0.0045422 ,  0.00825636, -0.00466264],\n",
       "        [-0.01299247, -0.01907639,  0.02088831, -0.00682283, -0.00351692,\n",
       "          0.01947091, -0.00564451,  0.00358695,  0.00815496, -0.004048  ],\n",
       "        [-0.01623194, -0.01731091,  0.0219022 , -0.00727887, -0.00311741,\n",
       "          0.02064045, -0.00538757,  0.00391987,  0.00779161, -0.00492743],\n",
       "        [-0.01685008, -0.01819069,  0.02213158, -0.00962351, -0.00115249,\n",
       "          0.02156216, -0.00575846,  0.00397823,  0.00930735, -0.00540409],\n",
       "        [-0.01751567, -0.01844294,  0.02200214, -0.00855631, -0.00086338,\n",
       "          0.02075243, -0.00740853,  0.00544625,  0.00879832, -0.0042123 ],\n",
       "        [-0.01665042, -0.01807391,  0.02103523, -0.00739427, -0.00207003,\n",
       "          0.02005798, -0.00547353,  0.00347028,  0.00947232, -0.00437363],\n",
       "        [-0.01611968, -0.01602046,  0.0220827 , -0.00624637, -0.00401147,\n",
       "          0.01977498, -0.00518768,  0.0025413 ,  0.00810423, -0.00491755],\n",
       "        [-0.01540377, -0.01840041,  0.02223385, -0.00556223, -0.00287239,\n",
       "          0.0209642 , -0.00642376,  0.00365838,  0.00846698, -0.00666085],\n",
       "        [-0.01472487, -0.01630414,  0.02141751, -0.00771362, -0.00319248,\n",
       "          0.02138747, -0.00647396,  0.00174881,  0.00916912, -0.00531383],\n",
       "        [-0.0133498 , -0.0191292 ,  0.02217479, -0.00505862, -0.00341354,\n",
       "          0.02060605, -0.00642608,  0.00213136,  0.00851   , -0.00604495],\n",
       "        [-0.01509618, -0.0188404 ,  0.02293654, -0.00733545, -0.00316884,\n",
       "          0.02066822, -0.00813712,  0.00409406,  0.00938958, -0.00451042],\n",
       "        [-0.01526004, -0.0184489 ,  0.02285996, -0.00587496, -0.00218391,\n",
       "          0.02102869, -0.00747567,  0.0026857 ,  0.00806696, -0.00539784],\n",
       "        [-0.01306646, -0.01762391,  0.02185963, -0.00803525, -0.00316141,\n",
       "          0.0191668 , -0.00540086,  0.00339979,  0.00826442, -0.00540276],\n",
       "        [-0.01498052, -0.01677015,  0.02198021, -0.00773367, -0.00257329,\n",
       "          0.02135081, -0.00606082,  0.00249497,  0.00836605, -0.00607359],\n",
       "        [-0.01470776, -0.01861701,  0.02201756, -0.00623387, -0.00206371,\n",
       "          0.02131032, -0.0067031 ,  0.00255228,  0.00859337, -0.00614808],\n",
       "        [-0.01640871, -0.01920206,  0.02237701, -0.00827579, -0.00275306,\n",
       "          0.02192649, -0.00772604,  0.005615  ,  0.00814759, -0.00370042],\n",
       "        [-0.01595518, -0.01836849,  0.02217774, -0.00841932, -0.00214385,\n",
       "          0.02187593, -0.00476962,  0.0045744 ,  0.00666229, -0.00563391],\n",
       "        [-0.01137179, -0.01987378,  0.02206102, -0.00785751, -0.00198963,\n",
       "          0.02017946, -0.00488864,  0.00290128,  0.00752565, -0.00668606],\n",
       "        [-0.01420513, -0.01821237,  0.02174241, -0.00666129, -0.00276144,\n",
       "          0.02057599, -0.00527997,  0.00440798,  0.00634888, -0.00595507],\n",
       "        [-0.01408943, -0.01827947,  0.02112837, -0.00500934, -0.00332625,\n",
       "          0.01969917, -0.00814677,  0.00342658,  0.00931888, -0.00472173],\n",
       "        [-0.01995867, -0.01573755,  0.02245672, -0.00712347, -0.00142508,\n",
       "          0.02219992, -0.00897563,  0.00453592,  0.0089951 , -0.00496728],\n",
       "        [-0.01271458, -0.01697341,  0.02239372, -0.00723075, -0.00286268,\n",
       "          0.02080002, -0.006821  ,  0.00170475,  0.00869458, -0.00699064],\n",
       "        [-0.01562959, -0.018108  ,  0.02070269, -0.00467682, -0.00101841,\n",
       "          0.01993865, -0.00590717,  0.00184312,  0.00832157, -0.00546603],\n",
       "        [-0.01580352, -0.01884794,  0.02202773, -0.00650426, -0.00220591,\n",
       "          0.0210596 , -0.00549385,  0.00483358,  0.00709933, -0.00616476],\n",
       "        [-0.01487395, -0.01947795,  0.02109042, -0.0053306 , -0.00193202,\n",
       "          0.02028814, -0.00731177,  0.00291994,  0.0079255 , -0.0032977 ],\n",
       "        [-0.01705059, -0.01859603,  0.02291194, -0.0068552 , -0.00349587,\n",
       "          0.02146223, -0.00622802,  0.00374556,  0.00938784, -0.00528185],\n",
       "        [-0.01799529, -0.01508243,  0.02283748, -0.00472394, -0.00376441,\n",
       "          0.0210089 , -0.00743458,  0.00255203,  0.0084842 , -0.00588196],\n",
       "        [-0.01549273, -0.01867992,  0.02254788, -0.00698415, -0.00225225,\n",
       "          0.02116527, -0.00671394,  0.00317963,  0.00849549, -0.00526529],\n",
       "        [-0.01441072, -0.0174695 ,  0.02204438, -0.00790389, -0.00414583,\n",
       "          0.02151961, -0.00803858,  0.00487143,  0.00881817, -0.00528507],\n",
       "        [-0.01458253, -0.01631783,  0.02263431, -0.00759708, -0.00286334,\n",
       "          0.0201445 , -0.00817302,  0.00415907,  0.00844895, -0.00585302],\n",
       "        [-0.01483492, -0.0191848 ,  0.02184322, -0.00695499, -0.00201108,\n",
       "          0.02114812, -0.00754371,  0.00472566,  0.00834625, -0.00553374],\n",
       "        [-0.01545456, -0.01756514,  0.02107152, -0.00662417, -0.00214946,\n",
       "          0.02051609, -0.00404866,  0.00234503,  0.00855639, -0.00664704],\n",
       "        [-0.01525198, -0.01657357,  0.02135265, -0.0083514 , -0.00267073,\n",
       "          0.02039552, -0.00583613,  0.00392392,  0.00822546, -0.00521375],\n",
       "        [-0.01415214, -0.01765697,  0.02064147, -0.00750735, -0.00204807,\n",
       "          0.01967223, -0.00570364,  0.00432006,  0.00667953, -0.00424512],\n",
       "        [-0.01353283, -0.01767769,  0.02049757, -0.00634883, -0.00430772,\n",
       "          0.01737507, -0.0030697 ,  0.00179088,  0.00981221, -0.00453896],\n",
       "        [-0.01566194, -0.01773232,  0.02101709, -0.00790801, -0.00180863,\n",
       "          0.02113202, -0.00628083,  0.0031858 ,  0.00963907, -0.00558224],\n",
       "        [-0.01782027, -0.01565335,  0.02278956, -0.0090611 , -0.00185128,\n",
       "          0.02200068, -0.00480815,  0.00325577,  0.00785707, -0.00670892],\n",
       "        [-0.01617746, -0.019764  ,  0.02143325, -0.00833412, -0.00178764,\n",
       "          0.02143573, -0.00515282,  0.00508845,  0.00774482, -0.0044862 ],\n",
       "        [-0.0166574 , -0.01752346,  0.02219499, -0.00694426, -0.00259171,\n",
       "          0.02080198, -0.00645903,  0.00335311,  0.0087623 , -0.00493652],\n",
       "        [-0.01358912, -0.01672718,  0.02135983, -0.0055183 , -0.00465617,\n",
       "          0.0200198 , -0.00811025,  0.00406515,  0.00789118, -0.00473493]]),\n",
       " 'b2': array([-0.03000162, -0.03592342,  0.04338113, -0.0141579 , -0.00512718,\n",
       "         0.04145956, -0.01226479,  0.00686103,  0.01678038, -0.01100718])}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class TwoLayerNet:\n",
    "## 1. 가중치 행렬 W1, W2, b1, b2 를 구성합니다.\n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):\n",
    "        # 가중치 초기화\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "        print('신경망이 생성되었습니다')\n",
    "\n",
    "#network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "#print(network.params.keys())  # 가중치 행렬을 가지고 있는 params 딕셔너리의 키 값들 확인\n",
    "                               # dict_keys(['W1', 'b1', 'W2', 'b2'])\n",
    "#print(network.params['W1'].shape)  # (784, 50)\n",
    "#print(network.params['W2'].shape)  # (50, 10)\n",
    "#print(network.params['b1'].shape)  # (50,)\n",
    "#print(network.params['b2'].shape)  # (10,)\n",
    "\n",
    "## 2. 입력데이터(필기체)를 넣고 1층과 2층을 거쳐서 확률벡터를 출력하는 함수\n",
    "    def predict(self, x):\n",
    "        W1, W2 = self.params['W1'], self.params['W2']   # 가중치 불러오는 코드\n",
    "        b1, b2 = self.params['b1'], self.params['b2']   # 편향 불러오는 코드\n",
    "    \n",
    "        a1 = np.dot(x, W1) + b1   # 1층 구성\n",
    "        z1 = sigmoid(a1)          # 1층 시그모이드 함수\n",
    "        a2 = np.dot(z1, W2) + b2  # 2층 구성\n",
    "        y = softmax(a2)           # 2층이 출력층이라 소프트맥스 함수\n",
    "        \n",
    "        return y\n",
    "\n",
    "#network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "#(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "#network.predict(x_train).shape  # (60000, 10)\n",
    "\n",
    "## 3. 오차(에러) 를 출력하는 함수\n",
    "    # x : 입력 데이터, t : 정답 레이블\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        \n",
    "        return cross_entropy_error(y, t)   # 오차의 평균을 출력\n",
    "    \n",
    "#network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "#(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "#network.loss(x_train[100], t_train[100])   # 2.298509143391436\n",
    "\n",
    "\n",
    "## 4. 정확도를 출력하는 함수\n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis=1)\n",
    "        t = np.argmax(t, axis=1)\n",
    "        \n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "        return accuracy\n",
    "\n",
    "#network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "#(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "#network.accuracy(x_train[:100,], t_train[:100,])   # 0.06\n",
    "\n",
    "## 5. 편미분해서 기울기를 출력하는 함수(4개의 기울기를 출력, W1, b1, W2, b2)\n",
    "    # x : 입력 데이터, t : 정답 레이블\n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_W = lambda W: self.loss(x, t)\n",
    "        \n",
    "        grads = {}\n",
    "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
    "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
    "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    "        \n",
    "        return grads\n",
    "\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "network.numerical_gradient(x_train[:100,], t_train[:100,])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. 위의 수치미분은 너무 느려서 못쓰고 오차역전파를 써서 가중치를 구해줘야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "신경망이 생성되었습니다\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'W2': array([[-2.26314243e-02, -2.01180584e-02,  2.00485577e-02,\n",
       "         -5.78204838e-03, -1.16740907e-03,  2.80987066e-02,\n",
       "         -7.39751592e-03,  2.83902603e-03,  1.07018240e-02,\n",
       "         -4.59165820e-03],\n",
       "        [-2.57605295e-02, -1.91532057e-02,  2.11914109e-02,\n",
       "         -5.09386617e-03, -3.94481905e-04,  2.87043854e-02,\n",
       "         -6.55216153e-03,  1.97842779e-03,  1.10842740e-02,\n",
       "         -6.00425321e-03],\n",
       "        [-1.86532432e-02, -2.00244182e-02,  1.85169615e-02,\n",
       "         -2.79358568e-03, -3.70552767e-03,  2.63387067e-02,\n",
       "         -5.63494781e-03,  1.57896437e-03,  1.04275732e-02,\n",
       "         -6.05048333e-03],\n",
       "        [-1.98973628e-02, -2.04122187e-02,  1.87762406e-02,\n",
       "         -7.46308573e-03, -2.56847980e-03,  2.59918511e-02,\n",
       "         -4.75338561e-03,  3.80740134e-03,  1.15963558e-02,\n",
       "         -5.07731634e-03],\n",
       "        [-2.21153497e-02, -1.77416417e-02,  1.96456966e-02,\n",
       "         -4.76374485e-03, -1.78905973e-03,  2.76433606e-02,\n",
       "         -8.41078482e-03,  2.22860186e-03,  1.16447478e-02,\n",
       "         -6.34182608e-03],\n",
       "        [-2.10899798e-02, -2.14255343e-02,  1.88119640e-02,\n",
       "         -5.85001359e-03, -1.96196550e-03,  2.82222395e-02,\n",
       "         -6.61558948e-03,  4.72385035e-03,  9.98465827e-03,\n",
       "         -4.79962938e-03],\n",
       "        [-2.28800317e-02, -1.82452253e-02,  2.05612837e-02,\n",
       "         -3.85836720e-03, -1.06412514e-03,  2.94672517e-02,\n",
       "         -9.33913485e-03,  2.83839428e-03,  9.78026695e-03,\n",
       "         -7.26031237e-03],\n",
       "        [-1.75483774e-02, -1.97503792e-02,  1.93127145e-02,\n",
       "         -6.45691897e-03, -2.32215473e-03,  2.68764483e-02,\n",
       "         -6.14976383e-03,  2.94396010e-03,  9.65230032e-03,\n",
       "         -6.55782909e-03],\n",
       "        [-1.58317843e-02, -2.19381738e-02,  1.94349280e-02,\n",
       "         -3.09140120e-03, -2.18266539e-03,  2.67061816e-02,\n",
       "         -9.60121348e-03,  1.31491131e-03,  1.13474590e-02,\n",
       "         -6.15824183e-03],\n",
       "        [-2.16484410e-02, -1.85141894e-02,  2.07961515e-02,\n",
       "         -4.15517739e-03, -1.26560033e-03,  2.69558716e-02,\n",
       "         -7.32076403e-03,  1.94245142e-03,  1.02672121e-02,\n",
       "         -7.05751447e-03],\n",
       "        [-1.97564140e-02, -1.92613730e-02,  1.89606944e-02,\n",
       "         -6.61180240e-03, -2.91535609e-03,  2.62625867e-02,\n",
       "         -6.38194187e-03,  2.33409974e-03,  1.21785423e-02,\n",
       "         -4.80903583e-03],\n",
       "        [-2.04482515e-02, -1.97615942e-02,  2.03328826e-02,\n",
       "         -3.44980491e-03, -1.66921661e-03,  2.68260901e-02,\n",
       "         -7.78945356e-03,  1.94839182e-03,  1.04907469e-02,\n",
       "         -6.47979066e-03],\n",
       "        [-2.05460062e-02, -2.06472326e-02,  2.07864784e-02,\n",
       "         -5.21695650e-03, -8.98779820e-04,  2.83567763e-02,\n",
       "         -5.69085613e-03,  1.32356315e-03,  9.94298778e-03,\n",
       "         -7.40997443e-03],\n",
       "        [-1.78773721e-02, -2.01819251e-02,  1.89646812e-02,\n",
       "         -5.14094965e-03, -2.59809670e-03,  2.62146029e-02,\n",
       "         -7.55273268e-03,  2.81716912e-03,  1.09449648e-02,\n",
       "         -5.59034182e-03],\n",
       "        [-2.01769876e-02, -1.90291918e-02,  2.07529566e-02,\n",
       "         -4.59218245e-03, -2.15966130e-03,  2.65633232e-02,\n",
       "         -4.84503255e-03,  1.68832565e-04,  1.03279626e-02,\n",
       "         -7.01001924e-03],\n",
       "        [-1.97904882e-02, -2.04531078e-02,  1.92310422e-02,\n",
       "         -6.13776830e-03, -1.72451442e-03,  2.78977974e-02,\n",
       "         -7.79339277e-03,  3.21689936e-03,  1.06950637e-02,\n",
       "         -5.14153112e-03],\n",
       "        [-2.29992927e-02, -1.87583082e-02,  2.05683262e-02,\n",
       "         -8.96399576e-03, -1.11704939e-03,  3.01430054e-02,\n",
       "         -9.28643734e-03,  3.69452803e-03,  1.25772873e-02,\n",
       "         -5.85806360e-03],\n",
       "        [-2.05771413e-02, -2.02993682e-02,  2.06741274e-02,\n",
       "         -4.22253755e-03, -2.02078177e-03,  2.89610427e-02,\n",
       "         -7.93481941e-03,  9.37705903e-04,  1.11691588e-02,\n",
       "         -6.68738661e-03],\n",
       "        [-2.07094634e-02, -2.03326466e-02,  1.95916673e-02,\n",
       "         -5.82554892e-03, -1.86321570e-03,  2.75610643e-02,\n",
       "         -5.30038691e-03,  2.22580672e-03,  1.10911296e-02,\n",
       "         -6.43840652e-03],\n",
       "        [-2.11476883e-02, -1.93010472e-02,  1.96866194e-02,\n",
       "         -3.86440995e-03, -3.12860591e-03,  2.85484638e-02,\n",
       "         -7.91012668e-03,  2.30931063e-03,  1.04225122e-02,\n",
       "         -5.61502796e-03],\n",
       "        [-2.06138799e-02, -2.08742711e-02,  1.95141373e-02,\n",
       "         -3.13493531e-03, -1.43136955e-03,  2.74557517e-02,\n",
       "         -9.35553795e-03,  3.37192786e-03,  1.04514200e-02,\n",
       "         -5.38324310e-03],\n",
       "        [-1.96737193e-02, -2.15349678e-02,  2.02283287e-02,\n",
       "         -6.73508598e-03, -1.21951511e-03,  2.80470642e-02,\n",
       "         -7.19181806e-03,  3.69665836e-03,  1.05261556e-02,\n",
       "         -6.14310071e-03],\n",
       "        [-2.29993928e-02, -1.95972522e-02,  2.08579781e-02,\n",
       "         -4.30370378e-03, -3.62040914e-03,  2.92455548e-02,\n",
       "         -7.79485417e-03,  1.30923717e-03,  1.25472742e-02,\n",
       "         -5.64443215e-03],\n",
       "        [-2.16925197e-02, -2.05554342e-02,  2.01712560e-02,\n",
       "         -6.00147377e-03, -1.15817551e-03,  2.78892167e-02,\n",
       "         -6.64603978e-03,  3.30088511e-03,  1.03107108e-02,\n",
       "         -5.61842551e-03],\n",
       "        [-2.41619308e-02, -2.01856675e-02,  2.09726276e-02,\n",
       "         -4.56924336e-03, -1.28887029e-03,  2.79405478e-02,\n",
       "         -7.69305420e-03,  3.70231136e-03,  1.03961941e-02,\n",
       "         -5.11291481e-03],\n",
       "        [-1.90376696e-02, -2.12567428e-02,  1.93549085e-02,\n",
       "         -3.85870576e-03, -6.74521709e-04,  2.73570664e-02,\n",
       "         -5.74859409e-03,  8.27457923e-04,  9.92361725e-03,\n",
       "         -6.88681614e-03],\n",
       "        [-1.84949019e-02, -2.10984586e-02,  1.94178894e-02,\n",
       "         -3.98451093e-03, -2.02669174e-03,  2.68407382e-02,\n",
       "         -7.28474986e-03,  1.46227084e-03,  1.11329914e-02,\n",
       "         -5.96457682e-03],\n",
       "        [-2.26259592e-02, -2.12830866e-02,  2.05655803e-02,\n",
       "         -4.86163710e-03, -2.74375150e-04,  2.88582926e-02,\n",
       "         -4.76173413e-03,  1.59515649e-03,  9.78277852e-03,\n",
       "         -6.99501577e-03],\n",
       "        [-2.26011239e-02, -2.07561681e-02,  1.99492930e-02,\n",
       "         -4.13225355e-03,  1.75738126e-04,  2.86449783e-02,\n",
       "         -7.11322094e-03,  1.78191737e-03,  1.13075147e-02,\n",
       "         -7.25667504e-03],\n",
       "        [-1.83231129e-02, -2.22502348e-02,  2.03801816e-02,\n",
       "         -6.34036531e-03, -1.77009144e-03,  2.87762570e-02,\n",
       "         -7.82312010e-03,  2.82536957e-03,  1.16966301e-02,\n",
       "         -7.17151379e-03],\n",
       "        [-1.79611532e-02, -2.13915690e-02,  2.00041893e-02,\n",
       "         -5.53357048e-03, -1.26442091e-03,  2.75169903e-02,\n",
       "         -7.05946477e-03,  1.84100326e-03,  9.52187846e-03,\n",
       "         -5.67388286e-03],\n",
       "        [-2.07368076e-02, -1.96797674e-02,  1.87043845e-02,\n",
       "         -6.35656726e-03,  3.74456726e-04,  2.69845403e-02,\n",
       "         -6.04727415e-03,  1.21470111e-03,  1.10147203e-02,\n",
       "         -5.47238657e-03],\n",
       "        [-1.64304469e-02, -1.93923736e-02,  1.78785557e-02,\n",
       "         -5.97424252e-03, -2.55519567e-03,  2.62760090e-02,\n",
       "         -7.89155380e-03,  2.69465996e-03,  1.09009706e-02,\n",
       "         -5.50638281e-03],\n",
       "        [-1.95725710e-02, -2.03052122e-02,  2.00963183e-02,\n",
       "         -3.18994347e-03, -6.05830980e-04,  2.74972073e-02,\n",
       "         -6.57270450e-03,  1.47830646e-03,  8.91283353e-03,\n",
       "         -7.73840349e-03],\n",
       "        [-1.63722197e-02, -1.84253098e-02,  1.92587479e-02,\n",
       "         -5.28076243e-03, -3.64856906e-03,  2.69466590e-02,\n",
       "         -7.99444059e-03,  1.33436212e-03,  1.07738371e-02,\n",
       "         -6.59230463e-03],\n",
       "        [-2.11674717e-02, -1.86468749e-02,  1.97372081e-02,\n",
       "         -4.22974121e-03, -2.39353570e-03,  2.79447112e-02,\n",
       "         -6.28533399e-03,  2.00717367e-03,  1.03354894e-02,\n",
       "         -7.30162492e-03],\n",
       "        [-2.01663190e-02, -2.04794134e-02,  1.90319686e-02,\n",
       "         -2.79705815e-03, -1.21057251e-03,  2.63312990e-02,\n",
       "         -7.04167418e-03,  3.10204526e-03,  9.20613455e-03,\n",
       "         -5.97641026e-03],\n",
       "        [-1.73355035e-02, -2.09392593e-02,  1.92688792e-02,\n",
       "         -4.26035910e-03, -2.23289068e-03,  2.42844590e-02,\n",
       "         -4.29988047e-03,  9.92235265e-04,  9.81368282e-03,\n",
       "         -5.29136326e-03],\n",
       "        [-2.26344152e-02, -2.11990310e-02,  1.99722830e-02,\n",
       "         -3.31193804e-03, -3.02125974e-03,  2.77943823e-02,\n",
       "         -6.05885060e-03,  2.04493577e-03,  1.19079526e-02,\n",
       "         -5.49405914e-03],\n",
       "        [-1.88000543e-02, -2.04391758e-02,  1.98774141e-02,\n",
       "         -5.80877865e-03, -4.50651517e-05,  2.81383611e-02,\n",
       "         -8.86114831e-03,  2.13635138e-03,  1.01954817e-02,\n",
       "         -6.39338613e-03],\n",
       "        [-2.07021461e-02, -1.90819571e-02,  2.00769648e-02,\n",
       "         -4.81872283e-03, -1.80467237e-03,  2.81320630e-02,\n",
       "         -8.79433447e-03,  1.25271479e-03,  1.08945899e-02,\n",
       "         -5.15449959e-03],\n",
       "        [-1.79620865e-02, -2.17033890e-02,  1.97319869e-02,\n",
       "         -4.14467038e-03, -1.07218509e-03,  2.74372349e-02,\n",
       "         -7.34039802e-03,  1.21335533e-03,  1.03118208e-02,\n",
       "         -6.47166894e-03],\n",
       "        [-2.43680495e-02, -1.92134207e-02,  2.18648565e-02,\n",
       "         -3.15593692e-03, -1.81643859e-03,  2.80215034e-02,\n",
       "         -8.50578546e-03,  1.93929079e-03,  1.09792267e-02,\n",
       "         -5.74524616e-03],\n",
       "        [-2.11564107e-02, -2.03607943e-02,  1.93799689e-02,\n",
       "         -5.34221795e-03, -1.20695728e-03,  2.83856450e-02,\n",
       "         -6.53873328e-03,  1.77051594e-03,  1.11037488e-02,\n",
       "         -6.03476514e-03],\n",
       "        [-2.07593813e-02, -1.97769466e-02,  1.95600444e-02,\n",
       "         -5.79175437e-03, -1.89463985e-03,  2.77422226e-02,\n",
       "         -6.10500507e-03,  1.93920510e-03,  1.14523691e-02,\n",
       "         -6.36611396e-03],\n",
       "        [-2.32719317e-02, -1.87117763e-02,  1.98636671e-02,\n",
       "         -4.97934867e-03, -1.15287583e-03,  2.72692169e-02,\n",
       "         -6.80299351e-03,  2.43095701e-03,  1.11294196e-02,\n",
       "         -5.77433451e-03],\n",
       "        [-1.70259168e-02, -1.89033199e-02,  1.81569233e-02,\n",
       "         -5.08759491e-03, -2.94746582e-03,  2.55563948e-02,\n",
       "         -6.10969462e-03,  2.10132872e-03,  9.47955329e-03,\n",
       "         -5.22020802e-03],\n",
       "        [-2.12017231e-02, -2.17000005e-02,  1.99339470e-02,\n",
       "         -5.04478989e-03, -2.00148619e-03,  2.90039501e-02,\n",
       "         -7.22214460e-03,  3.55567027e-03,  1.04150777e-02,\n",
       "         -5.73850078e-03],\n",
       "        [-2.09356045e-02, -2.05545105e-02,  1.91787109e-02,\n",
       "         -4.47176751e-03,  7.19118768e-05,  2.59497245e-02,\n",
       "         -5.05708048e-03,  2.76036599e-03,  7.49866901e-03,\n",
       "         -4.44041936e-03],\n",
       "        [-2.16362310e-02, -1.95698318e-02,  1.99556000e-02,\n",
       "         -6.37026945e-03, -1.32954734e-03,  2.76483743e-02,\n",
       "         -5.45606298e-03,  1.40513162e-03,  1.16539745e-02,\n",
       "         -6.30113799e-03]]),\n",
       " 'b2': array([-0.0399988 , -0.04044776,  0.03938955, -0.00981907, -0.00356505,\n",
       "         0.05469094, -0.01394415,  0.00437912,  0.02105394, -0.01173872]),\n",
       " 'W1': array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]]),\n",
       " 'b1': array([ 3.85206874e-04,  1.42858148e-04, -1.41712652e-04, -1.62881944e-04,\n",
       "         3.53322712e-05, -3.47091242e-05, -1.31942729e-04,  1.25317565e-04,\n",
       "         1.56412086e-04, -1.29084449e-05,  5.01259175e-04, -3.81350831e-06,\n",
       "         1.70121882e-05, -8.37548757e-05, -1.43557143e-04, -3.07906576e-04,\n",
       "         3.08829295e-04, -3.99379185e-04, -5.61410555e-04,  4.26113450e-04,\n",
       "         5.56442430e-04,  4.77401772e-04,  2.46335655e-04, -1.16546297e-04,\n",
       "         1.76626557e-04,  7.86191186e-05,  6.70237088e-04, -5.91969257e-06,\n",
       "         3.16943591e-04, -1.10306481e-04, -1.78587230e-05, -3.30073022e-04,\n",
       "         3.12753325e-05,  7.27796787e-05,  7.76450541e-05, -1.83513359e-04,\n",
       "         4.91511292e-04, -8.30572371e-05,  2.77623187e-04,  1.41546837e-04,\n",
       "         3.26861358e-04, -5.02450512e-05, -8.72026410e-05,  1.24092656e-04,\n",
       "         3.78614728e-04, -3.38962484e-05, -3.78769308e-04,  2.87439250e-04,\n",
       "         2.00978878e-04,  3.94457407e-05])}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class TwoLayerNet:\n",
    "## 1. 가중치 행렬 W1, W2, b1, b2 를 구성합니다.\n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):\n",
    "        # 가중치 초기화\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "        print('신경망이 생성되었습니다')\n",
    "\n",
    "#network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "#print(network.params.keys())  # 가중치 행렬을 가지고 있는 params 딕셔너리의 키 값들 확인\n",
    "                               # dict_keys(['W1', 'b1', 'W2', 'b2'])\n",
    "#print(network.params['W1'].shape)  # (784, 50)\n",
    "#print(network.params['W2'].shape)  # (50, 10)\n",
    "#print(network.params['b1'].shape)  # (50,)\n",
    "#print(network.params['b2'].shape)  # (10,)\n",
    "\n",
    "## 2. 입력데이터(필기체)를 넣고 1층과 2층을 거쳐서 확률벡터를 출력하는 함수\n",
    "    def predict(self, x):\n",
    "        W1, W2 = self.params['W1'], self.params['W2']   # 가중치 불러오는 코드\n",
    "        b1, b2 = self.params['b1'], self.params['b2']   # 편향 불러오는 코드\n",
    "    \n",
    "        a1 = np.dot(x, W1) + b1   # 1층 구성\n",
    "        z1 = sigmoid(a1)          # 1층 시그모이드 함수\n",
    "        a2 = np.dot(z1, W2) + b2  # 2층 구성\n",
    "        y = softmax(a2)           # 2층이 출력층이라 소프트맥스 함수\n",
    "        \n",
    "        return y\n",
    "\n",
    "#network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "#(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "#network.predict(x_train).shape  # (60000, 10)\n",
    "\n",
    "## 3. 오차(에러) 를 출력하는 함수\n",
    "    # x : 입력 데이터, t : 정답 레이블\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        \n",
    "        return cross_entropy_error(y, t)   # 오차의 평균을 출력\n",
    "    \n",
    "#network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "#(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "#network.loss(x_train[100], t_train[100])   # 2.298509143391436\n",
    "\n",
    "\n",
    "## 4. 정확도를 출력하는 함수\n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis=1)\n",
    "        t = np.argmax(t, axis=1)\n",
    "        \n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "        return accuracy\n",
    "\n",
    "#network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "#(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "#network.accuracy(x_train[:100,], t_train[:100,])   # 0.06\n",
    "\n",
    "## 5. 편미분해서 기울기를 출력하는 함수(4개의 기울기를 출력, W1, b1, W2, b2)\n",
    "    # x : 입력 데이터, t : 정답 레이블\n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_W = lambda W: self.loss(x, t)\n",
    "        \n",
    "        grads = {}\n",
    "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
    "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
    "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    "        \n",
    "        return grads\n",
    "\n",
    "#network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "#(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "#network.numerical_gradient(x_train[:100,], t_train[:100,])\n",
    "\n",
    "## 6. 위의 수치미분은 너무 느려서 못쓰고 5장에 배울 오차역전파를 써서 가중치를 구해줘야 합니다.\n",
    "## 지금 아래의 gradient 함수는 5장에서 배울 오차역전파 함수 입니다. 학습이 훨씬 빠릅니다.\n",
    "    def gradient(self, x, t):\n",
    "        W1, W2 = self.params['W1'], self.params['W2']\n",
    "        b1, b2 = self.params['b1'], self.params['b2']\n",
    "        grads = {}\n",
    "        \n",
    "        batch_num = x.shape[0]\n",
    "        \n",
    "        # forward\n",
    "        a1 = np.dot(x, W1) + b1\n",
    "        z1 = sigmoid(a1)\n",
    "        a2 = np.dot(z1, W2) + b2\n",
    "        y = softmax(a2)\n",
    "        \n",
    "        # backward\n",
    "        dy = (y - t) / batch_num\n",
    "        grads['W2'] = np.dot(z1.T, dy)\n",
    "        grads['b2'] = np.sum(dy, axis=0)\n",
    "        \n",
    "        da1 = np.dot(dy, W2.T)\n",
    "        dz1 = sigmoid_grad(a1) * da1\n",
    "        grads['W1'] = np.dot(x.T, dz1)\n",
    "        grads['b1'] = np.sum(dz1, axis=0)\n",
    "\n",
    "        return grads\n",
    "\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "network.gradient(x_train[:100,], t_train[:100,])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 2층 신경망 클래스를 가지고 학습시키는 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "신경망이 생성되었습니다\n",
      "train acc, test acc | 0.09751666666666667, 0.0974\n",
      "train acc, test acc | 0.7796, 0.7845\n",
      "train acc, test acc | 0.8755833333333334, 0.8777\n",
      "train acc, test acc | 0.89885, 0.9013\n",
      "train acc, test acc | 0.90845, 0.9114\n",
      "train acc, test acc | 0.9144333333333333, 0.9161\n",
      "train acc, test acc | 0.9192833333333333, 0.921\n",
      "train acc, test acc | 0.9246666666666666, 0.9271\n",
      "train acc, test acc | 0.9271833333333334, 0.9284\n",
      "train acc, test acc | 0.9304166666666667, 0.9315\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dataset.mnist import load_mnist\n",
    "#from two_layer_net import TwoLayerNet\n",
    "\n",
    "# 데이터 읽기\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "\n",
    "# 하이퍼파라미터\n",
    "iters_num = 6000  # 반복 횟수를 적절히 설정한다. 10에폭 돌게 설정 (60000/6000 = 10 에폭)\n",
    "train_size = x_train.shape[0]  # x_train.shape : (60000, 784)\n",
    "batch_size = 100   # 미니배치 크기 (60000/100 = 600 회가 1 에폭)\n",
    "learning_rate = 0.1  # 학습률\n",
    "\n",
    "train_loss_list = []  # 오차를 담을 리스트(시각화를 위해서 데이터를 저장)\n",
    "train_acc_list = []   # 훈련 데이터의 정확도를 담을 리스트(시각화를 위해서 데이터를 저장)\n",
    "test_acc_list = []    # 테스트 데이터의 정확도를 담을 리스트(시각화를 위해서 데이터를 저장)\n",
    "\n",
    "# 1에폭당 반복 수\n",
    "iter_per_epoch = max(train_size / batch_size, 1)  # 60000 / 100 = 600\n",
    "# 1 에폭당 정확도를 시각화 하기위해 필요한 코드\n",
    "\n",
    "for i in range(iters_num):   # 6000\n",
    "    # 미니배치 획득                #60000      100    \n",
    "    batch_mask = np.random.choice(train_size, batch_size)  # 0~60000 미만 숫자에서 100개의 숫자를 랜덤추출\n",
    "    x_batch = x_train[batch_mask]  # 훈련 데이터 100개\n",
    "    t_batch = t_train[batch_mask]  # 훈련 데이터 라벨 100개\n",
    "    \n",
    "    # 기울기 계산\n",
    "    #grad = network.numerical_gradient(x_batch, t_batch)\n",
    "    grad = network.gradient(x_batch, t_batch)  # 기울기 100개를 구한다\n",
    "    \n",
    "    # 매개변수 갱신\n",
    "    for key in ('W1', 'b1', 'W2', 'b2'):   # 가중치와 바이어스를 갱신\n",
    "        network.params[key] -= learning_rate * grad[key]\n",
    "    \n",
    "    # 시각화\n",
    "    # 학습 경과 기록\n",
    "    loss = network.loss(x_batch, t_batch)  \n",
    "    train_loss_list.append(loss)  # 오차를 담는\n",
    "    \n",
    "    # 1에폭당 정확도 계산\n",
    "    if i % iter_per_epoch == 0:  # 1에폭돌때 아래의 코드를 실행해라 (% : 나눗셈 나머지 계산)\n",
    "        train_acc = network.accuracy(x_train, t_train)  # 훈련 데이터 정확도 출력 ?개\n",
    "        test_acc = network.accuracy(x_test, t_test)     # 테스트 데이터 정확도 출력 ?개\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "        print(\"train acc, test acc | \" + str(train_acc) + \", \" + str(test_acc))\n",
    "\n",
    "# 그래프 그리기\n",
    "markers = {'train': 'o', 'test': 's'}\n",
    "x = np.arange(len(train_acc_list))\n",
    "plt.plot(x, train_acc_list, label='train acc')\n",
    "plt.plot(x, test_acc_list, label='test acc', linestyle='--')\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문제87. TwoLayerNet 클래스를 객체와 시켜서 numerical_gradient 함수를 실행해서 기울기를 출력하시오 ! (입력데이터를 100개만 입력하시오)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "신경망이 생성되었습니다\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'W1': array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]]),\n",
       " 'b1': array([ 1.02375506e-04,  1.09414586e-04,  1.69640055e-04, -1.46569246e-04,\n",
       "         5.44403986e-04, -2.49730010e-04, -2.49143002e-04, -2.23913039e-04,\n",
       "        -4.02555889e-04, -3.29840333e-04,  2.81356776e-04, -4.22388990e-05,\n",
       "        -2.00224861e-04, -3.68015556e-04,  9.69477609e-06, -8.15738699e-05,\n",
       "         3.40890811e-04, -2.33091013e-05, -9.31482513e-05, -3.12380077e-05,\n",
       "         3.67968733e-04,  1.41252263e-04,  2.92268854e-05,  6.27679397e-05,\n",
       "         1.31564071e-05,  1.68727543e-04,  1.37655283e-04,  2.64519553e-04,\n",
       "        -4.39202541e-05, -1.09129172e-04, -1.28873698e-04,  1.72998438e-05,\n",
       "        -4.14648094e-05,  2.36368436e-05,  4.29116997e-04, -4.69372585e-05,\n",
       "        -1.78094837e-04,  1.73331618e-04,  1.00046103e-04,  2.07768971e-04,\n",
       "         2.93363134e-04, -9.78365722e-05, -1.10607310e-04, -2.90166224e-04,\n",
       "         3.95310784e-05,  9.29665234e-06, -2.71030531e-06,  1.77377490e-04,\n",
       "        -6.34639956e-04, -8.19100343e-08]),\n",
       " 'W2': array([[-1.63357703e-02, -1.93597397e-02,  1.96650229e-02,\n",
       "         -3.94028669e-03, -3.22232429e-03,  2.80068285e-02,\n",
       "         -9.60257499e-03, -2.55517643e-04,  7.66299082e-03,\n",
       "         -2.61862855e-03],\n",
       "        [-1.20838677e-02, -2.01500539e-02,  1.86647856e-02,\n",
       "         -2.89187119e-03, -4.92455973e-03,  2.45999246e-02,\n",
       "         -4.96340010e-03, -3.14502484e-03,  7.78091691e-03,\n",
       "         -2.88684954e-03],\n",
       "        [-1.37658633e-02, -1.97797690e-02,  1.90120967e-02,\n",
       "         -3.94735674e-03, -4.32606263e-03,  2.52980281e-02,\n",
       "         -5.50071204e-03, -6.70460401e-04,  6.94687884e-03,\n",
       "         -3.26677950e-03],\n",
       "        [-1.39261223e-02, -1.91370927e-02,  1.95941029e-02,\n",
       "         -2.50043868e-03, -5.70637113e-03,  2.77261861e-02,\n",
       "         -8.53241305e-03, -1.83100159e-03,  7.99126120e-03,\n",
       "         -3.67811054e-03],\n",
       "        [-1.41140734e-02, -1.90359122e-02,  1.86208718e-02,\n",
       "         -4.12300067e-03, -4.83223952e-03,  2.71973272e-02,\n",
       "         -7.87931955e-03, -9.80416988e-04,  7.70799309e-03,\n",
       "         -2.56122956e-03],\n",
       "        [-1.52114903e-02, -1.93349551e-02,  1.84662060e-02,\n",
       "         -5.21679125e-03, -4.32178518e-03,  2.76138059e-02,\n",
       "         -8.20500107e-03, -1.27372679e-04,  8.36653502e-03,\n",
       "         -2.02915114e-03],\n",
       "        [-1.37527362e-02, -1.77283876e-02,  1.77341499e-02,\n",
       "         -4.90730037e-03, -4.90334057e-03,  2.68676470e-02,\n",
       "         -9.43789152e-03, -4.35989758e-04,  9.12660142e-03,\n",
       "         -2.56275215e-03],\n",
       "        [-1.48221115e-02, -2.01373610e-02,  1.97138751e-02,\n",
       "         -4.27193612e-03, -3.89222557e-03,  2.69391955e-02,\n",
       "         -6.62823236e-03, -1.51394523e-03,  7.55410363e-03,\n",
       "         -2.94136226e-03],\n",
       "        [-1.30830391e-02, -1.91716526e-02,  1.82454526e-02,\n",
       "         -4.75649412e-03, -5.17745390e-03,  2.43537243e-02,\n",
       "         -3.50160670e-03, -6.68711004e-04,  6.63512934e-03,\n",
       "         -2.87534858e-03],\n",
       "        [-1.13981213e-02, -1.96732029e-02,  1.87402665e-02,\n",
       "         -4.93511383e-03, -5.99466476e-03,  2.63110512e-02,\n",
       "         -7.30458087e-03, -2.02790201e-03,  9.34782871e-03,\n",
       "         -3.06556061e-03],\n",
       "        [-1.92889433e-02, -1.71126013e-02,  2.00837186e-02,\n",
       "         -4.70565487e-03, -6.06183454e-03,  2.84830971e-02,\n",
       "         -8.15687770e-03, -8.85307885e-04,  8.88491017e-03,\n",
       "         -1.24050611e-03],\n",
       "        [-1.47524708e-02, -1.82690943e-02,  1.74098392e-02,\n",
       "         -3.79858482e-03, -6.33065491e-03,  2.61107916e-02,\n",
       "         -6.86115530e-03, -5.35177234e-04,  9.17428785e-03,\n",
       "         -2.14778117e-03],\n",
       "        [-1.29652394e-02, -2.12202433e-02,  1.97283817e-02,\n",
       "         -4.63107503e-03, -5.16083656e-03,  2.74068882e-02,\n",
       "         -7.01791215e-03, -1.12561137e-03,  7.51011110e-03,\n",
       "         -2.52446302e-03],\n",
       "        [-1.34035616e-02, -1.92719255e-02,  1.80455245e-02,\n",
       "         -5.56585536e-03, -4.36429215e-03,  2.69173455e-02,\n",
       "         -6.26086541e-03, -6.48627307e-04,  7.51244310e-03,\n",
       "         -2.96018566e-03],\n",
       "        [-1.50084761e-02, -1.98456526e-02,  1.89358516e-02,\n",
       "         -7.00078730e-03, -3.44728319e-03,  2.67760849e-02,\n",
       "         -9.63621628e-03,  1.48674794e-03,  9.35719484e-03,\n",
       "         -1.61746357e-03],\n",
       "        [-1.69952413e-02, -1.87626646e-02,  1.91840940e-02,\n",
       "         -4.82521268e-03, -4.03187828e-03,  2.66531119e-02,\n",
       "         -9.16852791e-03,  6.20989058e-04,  8.61349030e-03,\n",
       "         -1.28816037e-03],\n",
       "        [-1.20772431e-02, -1.83525785e-02,  1.93609344e-02,\n",
       "         -1.46953495e-03, -5.46037629e-03,  2.71736939e-02,\n",
       "         -9.70915898e-03, -3.98750498e-03,  8.95197891e-03,\n",
       "         -4.43021029e-03],\n",
       "        [-1.62171466e-02, -1.71361714e-02,  1.96754046e-02,\n",
       "         -5.02809449e-03, -5.29032374e-03,  2.64345437e-02,\n",
       "         -7.95265391e-03, -1.02619961e-03,  8.75762683e-03,\n",
       "         -2.21698514e-03],\n",
       "        [-1.48252269e-02, -1.92019408e-02,  2.01426322e-02,\n",
       "         -6.29186256e-03, -2.29748919e-03,  2.72449405e-02,\n",
       "         -1.10711404e-02,  1.16397781e-03,  7.47442261e-03,\n",
       "         -2.33831308e-03],\n",
       "        [-1.33419317e-02, -1.88516869e-02,  1.86928619e-02,\n",
       "         -2.49241306e-03, -3.85342486e-03,  2.58187132e-02,\n",
       "         -8.00009203e-03, -6.47855047e-04,  6.25858407e-03,\n",
       "         -3.58275548e-03],\n",
       "        [-1.20847185e-02, -1.85939367e-02,  1.79003586e-02,\n",
       "         -4.43078966e-03, -5.83228159e-03,  2.67699084e-02,\n",
       "         -9.17876835e-03, -9.55081125e-04,  9.24939805e-03,\n",
       "         -2.84408898e-03],\n",
       "        [-1.47084629e-02, -1.90344899e-02,  1.81621449e-02,\n",
       "         -6.79752458e-03, -3.10528706e-03,  2.76455983e-02,\n",
       "         -7.19316840e-03, -2.57736401e-04,  7.96658424e-03,\n",
       "         -2.67765818e-03],\n",
       "        [-1.66426627e-02, -1.99962667e-02,  1.97710168e-02,\n",
       "         -4.33264866e-03, -5.05227433e-03,  2.81914207e-02,\n",
       "         -8.99730347e-03, -1.27736947e-03,  9.86339102e-03,\n",
       "         -1.52730301e-03],\n",
       "        [-1.39963438e-02, -1.93207586e-02,  1.94565471e-02,\n",
       "         -4.62663863e-03, -2.79676565e-03,  2.84388007e-02,\n",
       "         -7.45713412e-03, -8.40569379e-04,  6.24953961e-03,\n",
       "         -5.10667700e-03],\n",
       "        [-1.56680815e-02, -1.67278444e-02,  1.90108875e-02,\n",
       "         -4.09713501e-03, -4.05936367e-03,  2.65966491e-02,\n",
       "         -9.88687876e-03, -9.86491857e-04,  8.57766969e-03,\n",
       "         -2.75941098e-03],\n",
       "        [-1.45694894e-02, -1.95083131e-02,  1.97467983e-02,\n",
       "         -5.82922559e-03, -5.49585541e-03,  2.81105824e-02,\n",
       "         -8.01600468e-03, -4.05426159e-05,  8.22504882e-03,\n",
       "         -2.62299850e-03],\n",
       "        [-1.45301986e-02, -1.80786587e-02,  1.92894896e-02,\n",
       "         -2.37464498e-03, -5.90742873e-03,  2.59724397e-02,\n",
       "         -6.62888803e-03, -2.14796373e-03,  7.49199310e-03,\n",
       "         -3.08613946e-03],\n",
       "        [-1.19838449e-02, -1.92084572e-02,  1.68762430e-02,\n",
       "         -5.14327412e-03, -2.96871430e-03,  2.48398772e-02,\n",
       "         -6.49398052e-03, -1.37301177e-03,  7.92248419e-03,\n",
       "         -2.46732148e-03],\n",
       "        [-1.10311916e-02, -1.94638196e-02,  1.88217272e-02,\n",
       "         -6.82305743e-03, -5.10246711e-03,  2.69969301e-02,\n",
       "         -7.75140457e-03, -1.23077001e-03,  8.98540373e-03,\n",
       "         -3.40135055e-03],\n",
       "        [-1.65178378e-02, -1.93486356e-02,  1.90840618e-02,\n",
       "         -3.76044134e-03, -3.44275677e-03,  2.58011615e-02,\n",
       "         -7.51992362e-03, -1.10628120e-04,  7.85021895e-03,\n",
       "         -2.03521890e-03],\n",
       "        [-1.43338538e-02, -1.91108801e-02,  1.86104500e-02,\n",
       "         -3.92841121e-03, -4.30739187e-03,  2.60105618e-02,\n",
       "         -8.02983251e-03, -1.61308391e-03,  8.60316503e-03,\n",
       "         -1.90072335e-03],\n",
       "        [-1.74449916e-02, -1.84139189e-02,  1.88598696e-02,\n",
       "         -6.26561057e-03, -4.83417814e-03,  2.68692404e-02,\n",
       "         -7.24811524e-03,  3.51225802e-04,  9.02341451e-03,\n",
       "         -8.96935772e-04],\n",
       "        [-1.19118211e-02, -1.78398679e-02,  1.86868037e-02,\n",
       "         -3.76447590e-03, -6.62629430e-03,  2.67194875e-02,\n",
       "         -9.14874369e-03, -2.06472148e-03,  9.13312453e-03,\n",
       "         -3.18349124e-03],\n",
       "        [-1.64925794e-02, -1.94293496e-02,  1.92771865e-02,\n",
       "         -4.24161617e-03, -3.88655682e-03,  2.85393893e-02,\n",
       "         -8.34811318e-03, -1.64346011e-03,  8.13025293e-03,\n",
       "         -1.90515326e-03],\n",
       "        [-1.58727650e-02, -1.88202462e-02,  1.89063611e-02,\n",
       "         -2.95119784e-03, -4.04021513e-03,  2.68077211e-02,\n",
       "         -1.16542071e-02,  1.42632016e-03,  8.43902644e-03,\n",
       "         -2.24079730e-03],\n",
       "        [-1.11950067e-02, -1.78359680e-02,  1.79479092e-02,\n",
       "         -3.91601933e-03, -4.56198190e-03,  2.46812870e-02,\n",
       "         -8.46188557e-03, -8.25429789e-04,  7.28248618e-03,\n",
       "         -3.11539094e-03],\n",
       "        [-1.57597010e-02, -1.85712379e-02,  1.93970110e-02,\n",
       "         -3.81322627e-03, -5.37985752e-03,  2.76383578e-02,\n",
       "         -7.14240946e-03, -1.86128957e-03,  8.35672688e-03,\n",
       "         -2.86437366e-03],\n",
       "        [-1.45407085e-02, -2.05991398e-02,  1.88463955e-02,\n",
       "         -4.15997709e-03, -3.10197409e-03,  2.67056471e-02,\n",
       "         -8.83261770e-03, -1.61868793e-03,  9.24945691e-03,\n",
       "         -1.94839423e-03],\n",
       "        [-1.32308169e-02, -1.93317235e-02,  1.82108128e-02,\n",
       "         -2.55758401e-03, -4.59211432e-03,  2.65258753e-02,\n",
       "         -7.97098697e-03, -1.84521569e-03,  8.47482466e-03,\n",
       "         -3.68307120e-03],\n",
       "        [-1.86353733e-02, -1.75605969e-02,  1.82396626e-02,\n",
       "         -7.16553657e-03, -1.88360900e-03,  2.72249173e-02,\n",
       "         -5.82992347e-03,  4.42003687e-04,  7.32013525e-03,\n",
       "         -2.15167952e-03],\n",
       "        [-1.44165869e-02, -1.88195758e-02,  1.97754880e-02,\n",
       "         -4.71104973e-03, -5.96183138e-03,  2.76718778e-02,\n",
       "         -7.86173258e-03, -2.21084302e-03,  8.72270489e-03,\n",
       "         -2.18845104e-03],\n",
       "        [-1.28148684e-02, -1.88625701e-02,  1.79269767e-02,\n",
       "         -5.33786974e-03, -4.58069652e-03,  2.62513925e-02,\n",
       "         -7.58895950e-03, -4.65557097e-04,  7.30798497e-03,\n",
       "         -1.83583258e-03],\n",
       "        [-1.00752618e-02, -2.06888670e-02,  1.83372944e-02,\n",
       "         -4.48190874e-03, -5.95474200e-03,  2.64004829e-02,\n",
       "         -7.26875278e-03, -9.62994993e-04,  7.68182189e-03,\n",
       "         -2.98707170e-03],\n",
       "        [-1.70142658e-02, -1.78245705e-02,  1.92569245e-02,\n",
       "         -4.56089506e-03, -4.98219585e-03,  2.84449472e-02,\n",
       "         -7.59694709e-03, -1.77639153e-03,  8.45866988e-03,\n",
       "         -2.40527562e-03],\n",
       "        [-1.38945990e-02, -1.87270632e-02,  1.89820567e-02,\n",
       "         -3.63276927e-03, -5.70902631e-03,  2.79211275e-02,\n",
       "         -8.96911609e-03, -1.31820983e-03,  8.46385795e-03,\n",
       "         -3.11625820e-03],\n",
       "        [-1.79109279e-02, -1.79371427e-02,  1.82604834e-02,\n",
       "         -3.24790403e-03, -4.13380809e-03,  2.67880791e-02,\n",
       "         -6.29360694e-03, -1.35840067e-03,  8.35306509e-03,\n",
       "         -2.51983720e-03],\n",
       "        [-1.57104419e-02, -1.91433860e-02,  1.86668176e-02,\n",
       "         -5.52886237e-03, -3.78431457e-03,  2.69487682e-02,\n",
       "         -6.92163407e-03, -1.34537911e-03,  8.28296575e-03,\n",
       "         -1.46453339e-03],\n",
       "        [-1.55183686e-02, -1.73381806e-02,  1.82044441e-02,\n",
       "         -3.69684418e-03, -2.81520807e-03,  2.69801033e-02,\n",
       "         -1.05116206e-02, -8.25691004e-04,  8.03160486e-03,\n",
       "         -2.51023912e-03],\n",
       "        [-1.31851338e-02, -2.09073968e-02,  1.84243555e-02,\n",
       "         -5.29004613e-03, -3.92344499e-03,  2.63362355e-02,\n",
       "         -6.94899093e-03, -1.39212232e-03,  8.68123645e-03,\n",
       "         -1.79469242e-03],\n",
       "        [-1.68604228e-02, -2.06582457e-02,  1.98272113e-02,\n",
       "         -5.77645123e-03, -4.74950897e-03,  2.86945568e-02,\n",
       "         -8.35722978e-03,  1.30079389e-03,  8.33363838e-03,\n",
       "         -1.75434165e-03]]),\n",
       " 'b2': array([-0.02921832, -0.03825767,  0.03767323, -0.00914321, -0.00932442,\n",
       "         0.05382185, -0.01553321, -0.0018409 ,  0.01678421, -0.00496157])}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\n",
    "from common.functions import *\n",
    "from common.gradient import numerical_gradient\n",
    "from dataset.mnist import load_mnist\n",
    "\n",
    "class TwoLayerNet:\n",
    "## 1. 가중치 행렬 W1, W2, b1, b2 를 구성합니다.\n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):\n",
    "        # 가중치 초기화\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "        print('신경망이 생성되었습니다')\n",
    "\n",
    "\n",
    "## 2. 입력데이터(필기체)를 넣고 1층과 2층을 거쳐서 확률벡터를 출력하는 함수\n",
    "    def predict(self, x):\n",
    "        W1, W2 = self.params['W1'], self.params['W2']   # 가중치 불러오는 코드\n",
    "        b1, b2 = self.params['b1'], self.params['b2']   # 편향 불러오는 코드\n",
    "    \n",
    "        a1 = np.dot(x, W1) + b1   # 1층 구성\n",
    "        z1 = sigmoid(a1)          # 1층 시그모이드 함수\n",
    "        a2 = np.dot(z1, W2) + b2  # 2층 구성\n",
    "        y = softmax(a2)           # 2층이 출력층이라 소프트맥스 함수\n",
    "        \n",
    "        return y\n",
    "\n",
    "## 3. 오차(에러) 를 출력하는 함수\n",
    "    # x : 입력 데이터, t : 정답 레이블\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        \n",
    "        return cross_entropy_error(y, t)   # 오차의 평균을 출력\n",
    "\n",
    "# 4. 정확도를 출력하는 함수\n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis=1)\n",
    "        t = np.argmax(t, axis=1)\n",
    "        \n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "        return accuracy\n",
    "\n",
    "# 5. 편미분해서 기울기를 출력하는 함수(4개의 기울기를 출력, W1, b1, W2, b2)\n",
    "    # x : 입력 데이터, t : 정답 레이블\n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_W = lambda W: self.loss(x, t)\n",
    "        \n",
    "        grads = {}\n",
    "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
    "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
    "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    "        \n",
    "        return grads\n",
    "\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "network.numerical_gradient(x_train[:100,], t_train[:100,])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문제88. 10에폭이 아니라 20에폭 돌게 코드를 수정하시오"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "신경망이 생성되었습니다\n",
      "train acc, test acc | 0.10218333333333333, 0.101\n",
      "train acc, test acc | 0.7866666666666666, 0.7867\n",
      "train acc, test acc | 0.8762, 0.8803\n",
      "train acc, test acc | 0.8989833333333334, 0.9025\n",
      "train acc, test acc | 0.9073, 0.9093\n",
      "train acc, test acc | 0.91395, 0.9161\n",
      "train acc, test acc | 0.9192666666666667, 0.9215\n",
      "train acc, test acc | 0.9238333333333333, 0.9252\n",
      "train acc, test acc | 0.9274333333333333, 0.929\n",
      "train acc, test acc | 0.9303666666666667, 0.9317\n",
      "train acc, test acc | 0.9337666666666666, 0.933\n",
      "train acc, test acc | 0.93745, 0.9367\n",
      "train acc, test acc | 0.9383333333333334, 0.9375\n",
      "train acc, test acc | 0.9415166666666667, 0.9405\n",
      "train acc, test acc | 0.94335, 0.9439\n",
      "train acc, test acc | 0.94535, 0.9458\n",
      "train acc, test acc | 0.9474666666666667, 0.9457\n",
      "train acc, test acc | 0.9496166666666667, 0.9474\n",
      "train acc, test acc | 0.9508666666666666, 0.9483\n",
      "train acc, test acc | 0.9519666666666666, 0.9498\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dataset.mnist import load_mnist\n",
    "#from two_layer_net import TwoLayerNet\n",
    "\n",
    "# 데이터 읽기\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "\n",
    "# 하이퍼파라미터\n",
    "iters_num = 6000 * 2 # 반복 횟수를 적절히 설정한다. 10에폭 돌게 설정\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100   # 미니배치 크기\n",
    "learning_rate = 0.1  # 학습률\n",
    "\n",
    "train_loss_list = []  # 오차를 담을 리스트(시각화를 위해서 데이터를 저장)\n",
    "train_acc_list = []  # 훈련 데이터의 정확도를 담을 리스트(시각화를 위해서 데이터를 저장)\n",
    "test_acc_list = []  # 테스트 데이터의 정확도를 담을 리스트(시각화를 위해서 데이터를 저장)\n",
    "\n",
    "# 1에폭당 반복 수\n",
    "iter_per_epoch = max(train_size / batch_size, 1)  # 60000 / 100 = 600\n",
    "# 1 에폭당 정확도를 시각화 하기위해 필요한 코드\n",
    "\n",
    "for i in range(iters_num):   # 60000\n",
    "    # 미니배치 획득                #60000      100    \n",
    "    batch_mask = np.random.choice(train_size, batch_size)  # 0~60000 미만 숫자에서 100개의 숫자를 랜덤추출\n",
    "    x_batch = x_train[batch_mask]  # 훈련 데이터 100개\n",
    "    t_batch = t_train[batch_mask]  # 훈련 데이터 라벨 100개\n",
    "    \n",
    "    # 기울기 계산\n",
    "    #grad = network.numerical_gradient(x_batch, t_batch)\n",
    "    grad = network.gradient(x_batch, t_batch)  # 기울기 100개를 구한다\n",
    "    \n",
    "    # 매개변수 갱신\n",
    "    for key in ('W1', 'b1', 'W2', 'b2'):   # 가중치와 바이어스를 갱신\n",
    "        network.params[key] -= learning_rate * grad[key]\n",
    "    \n",
    "    # 시각화\n",
    "    # 학습 경과 기록\n",
    "    loss = network.loss(x_batch, t_batch)  \n",
    "    train_loss_list.append(loss)  # 오차를 담는\n",
    "    \n",
    "    # 1에폭당 정확도 계산\n",
    "    if i % iter_per_epoch == 0:  # 1에폭돌때 아래의 코드를 실행해라\n",
    "        train_acc = network.accuracy(x_train, t_train)  # 훈련 데이터 정확도 출력 ?개\n",
    "        test_acc = network.accuracy(x_test, t_test)     # 테스트 데이터 정확도 출력 ?개\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "        print(\"train acc, test acc | \" + str(train_acc) + \", \" + str(test_acc))\n",
    "\n",
    "# 그래프 그리기\n",
    "markers = {'train': 'o', 'test': 's'}\n",
    "x = np.arange(len(train_acc_list))\n",
    "plt.plot(x, train_acc_list, label='train acc')\n",
    "plt.plot(x, test_acc_list, label='test acc', linestyle='--')\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문제89. 4장에서 만든 2층 신경망의 가중치와 바이어스를 pickle 파일로 내리시오"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "신경망이 생성되었습니다\n",
      "train acc, test acc | 0.10441666666666667, 0.1028\n",
      "train acc, test acc | 0.7923, 0.7991\n",
      "train acc, test acc | 0.8775333333333334, 0.8821\n",
      "train acc, test acc | 0.8977, 0.9006\n",
      "train acc, test acc | 0.9074833333333333, 0.9108\n",
      "train acc, test acc | 0.9146, 0.9172\n",
      "train acc, test acc | 0.9185166666666666, 0.9199\n",
      "train acc, test acc | 0.9234166666666667, 0.9244\n",
      "train acc, test acc | 0.92725, 0.928\n",
      "train acc, test acc | 0.9303, 0.932\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEKCAYAAAAfGVI8AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAAp+UlEQVR4nO3deXhU9b3H8fc3s2UBQhaQLUhEkE1ZRVS0LpWC1rq0bnVpbd1qte29rVds61qv9Wrrtb11o9YuarVudWmpUi3ugoAiqwoCkgBCgCQQss3yu3/MSEMIMGhOzoT5vJ4nTzJzzsx8Zh44nznb75hzDhERyV45fgcQERF/qQhERLKcikBEJMupCEREspyKQEQky6kIRESynGdFYGYPmNkGM1u0i+lmZr82s+VmtsDMxniVRUREds3LNYI/AJN3M30KMCj1cwlwj4dZRERkFzwrAufcq8Dm3cxyCvAnlzQL6G5mvb3KIyIibQv6+Np9gYoWtytT961rPaOZXUJyrYGCgoKxQ4YM6ZCAIiL7innz5m10zvVoa5qfRWBt3NfmeBfOuWnANIBx48a5uXPneplLRGSfY2Yf72qan0cNVQJlLW73A9b6lEVEJGv5WQTPAhekjh6aANQ653baLCQiIt7ybNOQmT0CHAOUmlklcD0QAnDO3QtMB04ElgP1wIVeZRERkV3zrAicc+fsYboDvuvV64uISHp0ZrGISJZTEYiIZDkVgYhIllMRiIhkORWBiEiWUxGIiGQ5FYGISJZTEYiIZDk/B50TEdm3JBIQb4JYE8SjkNcdAiGo3wy1Fcn7Yk0Qb8bFmoj1P4JYoID4+sVYxRwSsabkT7SJRKyZTQdfRHOggLxVLxLe/AEc+QPKivPbPbaKQEQyn3PbF6DEo6nfzZDbDfKKcNEG4usWEm1uIh5tIhZtIh5tpr77QTR0KSNet4mCj/5GItqMizeTiCV/1vY6jk1dBhOuXcWgFX/c/twWb8YSUd7sdR6r8obRa8sCJlf+HzkuSjDRTI6LEXTNTCu9hiWh4Yype5XvVd9CgMQOsS/NvY1FDOLE6Ax+Et/x2lsGTG66nY9cX74V+AfXhR7c6W2f/Nr+rKWUiwIvMTFnEbOCpzN1SvsPw68iEJG2JeKQiEEwkrxdWwnN9f/+xhtrwuV2o7l0GE2xBCz6K7GGLcSjjcSbG0lEG9na9UA+6XM8jdE4g+fdCNF6iDVDrAmLN/FR9yOY0+OrxJob+PbSbxNINBNMNBNwyd/TC07lkfxzyY1W8+fqc3eKeKc7h7tjp9AzsZ7XI9/faYH2q+g3+FP8Sxxkq3kh8pOdHj/tvSYejzcw0pbzu/DzRAkSdYHkb4LMrFrJvJxuHBKoYaQLErM8YhYibiHiOSEqGsJsjceozOnDs13OxOWESOSESeSEcYEQPQoHMiFcQjB+LA83lUEgDMEwBMJYIMLpXQdh4TzyE2X8xZ1HTihCTjD1Ewrz02CQYI4RChxKIGCcXdT+awMAlhzyp/PQ9Qik04rHILotuTCNfvrTCGWHJqdXvA1V7ye/lSZiyR8LwITLAHALnySxbgHxWJREPEoiFiUWLKBqwjVE4wkK591FpGoBxGO4eBSXiNEQ6cE7o2+mKZZgzPzrKKpdAokY5mJYIk5VXjl/Gfg/ROMJvvnh5fRpWEaOixNwMQLEWRgexfXdb6EpluC31RfRx32yw1v6Z3wsF0d/CMDcyGWU2pYdpv81fiT/EU0OKfZ65HsYjmYXpIkwzQT5e3wCv7dTyA3Cr+wOYhYmnpP8SQTCLM4dy+Iuh5NnMSbXPZVckAZC239XdRlKddfB5NPEgG3vkhMIY6mFaCAYIdqlN+SXErYY+bEagqEIwXAk+TsUJhIOEQrkEArkEA6mfgdyCAWMYGDf2oVqZvOcc+PanKYiEEmJR6Fpa3IB3XJh3XsUhPNh/RKomAXRhh2nH/tjEuFuNL/7KDnv/ZlE87bt81isgVe/9A+2xCMMXfBzhn380E4ve0n5P6mPOi7YeAeTGp/fYdo2chmf+CPN8QS35/wfJ+bMJkaQGDnECLDeFTOl+VYAbgnez6E5HxAjQIwc4gSocD24Mvo9AKYGH2GgrSFOYPs8lfTmHjuTUDCHb/MsPa0GlxOEnCCJQJiNob7M6fZFIsEcxje+SZ5FsVAEC+ZiwQjRvFLqCgcTCeZQEl1HKBQiEM4lFM4jGMklHM4lEgoSCeaQG8ohEgwQCaZ+h5IL3Zyctq5RJe1NRSDZIZGAplpoqIHGmtTvWug/Abr2gnXvwdzfQ2MNiYYaEvXVuIZaVh53FxvyD6Lw/Uc4eN5Pd3raXw3+E6sC+3PYhsc4e9Nd2++PEqSRMKcmbuej5iLOCLzM2YGZ1LsIjUSoJ0KDC3NT7ALqyWVCzhKG2yoaiBAP5JEI5pIIFbAkdwz5uWF6BbbQNRjHghGCwSA5wTDBYAjCBdu/rUaCyW+r4UAOoR3uy9l+XziQQzhohAMBQkHbPi0c3HGeUMAw00I4W6gIpPOIR/+9IG9MLdRLD4SiAbBlLcy6GxqSC/J4fQ2uoZq1Y37Imh5HE/z4NQ577Zs7PeWve97EazmHMnjLbH5Yfwe1roBal5/8TQG/ip3OR64vB1olE3MWUU+ERhehgTCxQB7LQ0NwkS70CDVTFGomGCkgJ1JAbiRCfiRIfihAfiRIQTiw/XZBJEB+OEh+OPm75e28UEDfgqXD7a4ItLNYOk48CqteT+50rK3A1VTQvHk1VeWnsHr/04luXMUX/nH8Tg97tPg7PBU+hcJtK/i/rfexxRVQ4/KppYBaV8DvnlvOW4kIPdnGlwPnU+sK2Gr5REOFJHILqYv2JZyXw8beR/Hz3OPolheiW26IbnlBCvNCXJMbomtukIJI6ie1QM8LBQhogS1ZQGsE0r7e/zvUVCQX9LUVxDevZlPvLzC3/DLWbqzm4lcPByCBscEVscaV8HDseJ5KHE0ejVwUmL59AR8NdyMWLmRLXhkuv5RuuUEK88PbF+LJ3yG65QZ3WLh3ywvRJRzUt26RFrRGIJ+Pc/DptuQlz8DGD1ML+0oSNaupKzmEuWN+TsXmBr72r+9SEKumkQhrXSmViWJmVNTy0FvvAPBG3k0EC/uSW1pG3+Ju9CvOZ1KXCF9LfTvvljsluSCPBPVtXKSDqAgkeYRLKC/595JnYO272xf0rraC5oI+zPvio1RsrucLL99Cr23vU2uFrKGUj2PFzPkknwcWJNfS/hL6KbmF+1FY3JP+JQWUFeczsSifc4rzKCvOp1vuST6+URFpi4ogG23bCK//L6x4GVdbQSKQy3tnzaZicz0Hv/Y7+m9+g005PVjjSlgZHcjSTWXc/9vZAPTOuZL8biX0LC6irDiPsqJ8DinO58nifMqK8+jRJaIjUUQ6GRVBtln0JO6ZK3HRBuYFR/Fh8wRWxUv47d1vAEYBF1BQcDl9irvSP7VwP7Aon4eL8ykryqd391xC+9iJNiLZTkWQDWJN0LyNxlAhT6/Kp7h5BLdFv0bZ/qMoL+1CWXEevyvOp6w4n35FeeSH9c9CJJvof/y+LBGHhU/gZt5MRf4Iztp0EetqE3xx6H9zz+QhDNqvq98JRSQDqAj2Rc7Bshnw4o2wYTEfBQ7gpg0H07NPhP89axQTDijxO6GIZBAVwb7orbtgxk9YH+zDzc1X8F7hsVz1lWF8+ZDe2pErIjtREewrNiyFRIzKyECmrRxGInohz9skLj9xKL+Y0J9IMOB3QhHJUCqCzq5mNcz8Oe69R1hReBhTNv0AgG9NvIyXjhlIYV7I33wikvFUBJ3Vto3w2i9xc+4n7uBhvsydG77MyaP78J+TBtO3e57fCUWkk1ARdFKJ+X/GZt3L3wPH8t/1p3LgoIN4aMoQhvcp9DuaiHQyKoLOItYM8/4AXXvxVuRIfvnOcKqb/odwr6H8z1lDOHpwD78TikgnpSLIdIkELHoC/nUz1HzMGwUncO6mAH0Kc/nh107k1NF9NTibiHwuKoJMtup1+MdUWL+QtbmD+HHz1cxjDFOnDOKbRwwgN6QjgUTk81MRZKLUsM8NNetpqtnMzxJX8tzWCZx3+AH873EHUlQQ9juhiOxDVASZZMP78NJNxHuP4s+RM7nzn13ZUv9zJo/sz4uTDqJ/Sb7fCUVkH6QiyAQ1FfBy8lyAWCCfaSt7cfuWxYwvL+bHJ45nVFl3vxOKyD7M0yIws8nAr4AAcL9z7tZW0wuBh4D+qSy/cM793stMGWfeH2D6f5Fwjr/lncr1mydR0rMPvzttCMcN6akhIUTEc54VgZkFgLuAE4BKYI6ZPeucW9Jitu8CS5xzJ5tZD+ADM3vYOdfsVa6M0FQH8WbIL2ZN+AA+zj+GH1WdRDTQl/86fTBnjO1HUGP+i0gH8XKNYDyw3Dm3AsDMHgVOAVoWgQO6WvJrbxdgMxDzMJP/KubAo1+n8YBJ3BL8Dn+eXUck+C0uPWEgFx1VrmsBiEiH83Kp0xeoaHG7Ejis1Ty/AZ4F1gJdgbOcc4nWT2RmlwCXAPTv39+TsB0lPnsa0aYmvr1gCLOiqzlnfBnfP34wPbpG/I4mIlnKyyJoa+O2a3X7S8B84DhgIPBPM3vNObdlhwc5Nw2YBjBu3LjWz9GpbF3+BrOaBlMw6HBmTBnCwB5d/I4kIlnOyw3RlUBZi9v9SH7zb+lC4CmXtBxYCQzxMJO/6qro3riGT7odzLQLxqkERCQjeFkEc4BBZlZuZmHgbJKbgVpaDRwPYGb7AQcBKzzM5Kvo2gUAuH7jfU4iIvJvnhWBcy4GXAG8ACwFHnPOLTazy8zsstRsPwOOMLOFwEvA1c65jV5l8tvivLGMbJzGfkOP9DuKiMh2nh6i4pybDkxvdd+9Lf5eC0zyMkMmeefjamrpwuhyjRQqIplDB6t3lHiUMW9dwaldP6B3oS4aIyKZQ0XQUT5ZyKhtrzOytFMf9CQi+yAVQQepXfYGAAUHHuFzEhGRHek01g6ydflb1Ltihh40zO8oIiI70BpBBynY8A7vMYghvbv6HUVEZAcqgo4QbWBtopg1hWMJaTA5EckwWip1gHoX4iv1P2XTsG/4HUVEZCcqgg6woKKGeMIxdv8iv6OIiOxEO4s7QN/nzuGWYAGj+5/gdxQRkZ1ojcBrsWZ61b5LMK8bxbrovIhkIBWBx9wnCwi5KI37jfU7iohIm1QEHtv0/usAFA7WiWQikplUBB6rX/EWa1wJww7ady+zICKdm3YWe2yOHcJK68F/6iI0IpKhVAQeu69uIr3755GT09aVO0VE/KdNQx7aUlXBxvVrdf6AiGQ0FYGHal+6gzcjVzKuTJuFRCRzqQg8FFgzh0WunEP21xXJRCRzqQi8Emuix9b3WZU3gi4R7YoRkcylIvBIfM18QkSJ9taJZCKS2VQEHqla+hoA3Q+a6HMSEZHdUxF45M3IkXyv+buMOGiw31FERHZLReCR1zbk8Wb+cfQryvM7iojIbqkIvFBXxX4fPc7R/QwznUgmIplNReCBLUv/xdTm3zCxtN7vKCIie6TjGj1Q/eHrhFyY/YeP9zuKiMgeaY3AA6G1c1noBjK8X6nfUURE9khF0N6ijfTY9iGVXUaQGwr4nUZEZI9UBO2sed1CQsSI9xnndxQRkbSoCNrZIg5kXOM9FI6Y5HcUEZG0qAja2TsfV7ORQkYd0MfvKCIiaVERtCfnGP72VM7otpie3XL9TiMikhYVQTtyNR9z+NYXGF+0ze8oIiJpUxG0o80fvAFA3gETfE4iIpI+T4vAzCab2QdmttzMpu5inmPMbL6ZLTazV7zM47XaD99gm4swYJhOJBORzsOzM4vNLADcBZwAVAJzzOxZ59ySFvN0B+4GJjvnVptZT6/ydITIurksYiBj++gaxSLSeXi5RjAeWO6cW+GcawYeBU5pNc/Xgaecc6sBnHMbPMzjrUSCmmiAym5jCAa0xU1EOg8vl1h9gYoWtytT97U0GCgys5fNbJ6ZXdDWE5nZJWY218zmVlVVeRT389kWTfCV+mtZdfD3/I4iIrJXvCyCtsZfdq1uB4GxwEnAl4BrzWynK7k456Y558Y558b16JGZF4J/r7KGeMIxZn9tFhKRziWtIjCzJ83sJDPbm+KoBMpa3O4HrG1jnuedc9uccxuBV4GRe/EaGaP0+e9wR+huxpSpCESkc0l3wX4Pye35y8zsVjMbksZj5gCDzKzczMLA2cCzreZ5BjjKzIJmlg8cBixNM1PmcI5em96mS26IwvyQ32lERPZKWkcNOedeBF40s0LgHOCfZlYB/BZ4yDkXbeMxMTO7AngBCAAPOOcWm9llqen3OueWmtnzwAIgAdzvnFvULu+sAyU2raRbooa6XmP8jiIistfSPnzUzEqA84DzgXeBh4GJwDeAY9p6jHNuOjC91X33trp9O3D73oTONBuWvkYvoGDgEX5HERHZa2kVgZk9BQwBHgROds6tS036i5nN9SpcZ1G3/E22ujwGDj/U7ygiInst3TWC3zjn/tXWBOdc1g+8/25sALU2mW/17OZ3FBGRvZbuzuKhqbOAATCzIjO73JtInc99W4/gzQHfJSenrSNmRUQyW7pFcLFzrubTG865auBiTxJ1MrWb1rNhwyeM1fkDItJJpVsEOWa2/etuahyhsDeROpdNL9/Lu5FLGddL1ycWkc4p3X0ELwCPmdm9JM8Ovgx43rNUnYirfJuV9GbEAWV7nllEJAOlWwRXA5cC3yE5dMQM4H6vQnUaztGjdiGzcw/lwIhnA7mKiHgq3RPKEiTPLr7H2zidS6xqOd0StTT0HOt3FBGRzyzd8wgGAT8HhgHbL8brnDvAo1ydwieLX6Uf0GWQTiQTkc4r3Z3Fvye5NhADjgX+RPLksqw22w3lmui3GTwi60+lEJFOLN0iyHPOvQSYc+5j59wNwHHexeocXt2Qx78KTqRvUYHfUUREPrN0i6AxNQT1MjO7wsxOAzr1ZSU/t6Y6Sj/6K1/oCy2OrBUR6XTSLYIfAPnA90heSOY8koPNZa3qZW9ybfRXHFv4id9RREQ+lz3uLE6dPHamc+4qoA640PNUnUDVktcpAvoMn+h3FBGRz2WPawTOuTgw1rT9Ywe2Zg7LXD+GlOtEMhHp3NI9C+pd4BkzexzY9umdzrmnPEmV6RIJem1ZwOy8IxkU1NASItK5pVsExcAmdjxSyAFZWQRNVcvp6upo6q0TyUSk80v3zGLtF2hhUWMplzf+hlsOOczvKCIin1u6Zxb/nuQawA6cc99q90SdwLyPq1lPMYccuL/fUUREPrd0Nw39rcXfucBpwNr2j9M5HDDvvzm3cBA9up7kdxQRkc8t3U1DT7a8bWaPAC96kijDuYYajqt9iljpN/2OIiLSLtI9oay1QUD/9gzSWVS9/yY5OAIDDvc7iohIu0h3H8FWdtxH8AnJaxRknU3vv06pM8pG6EQyEdk3pLtpqKvXQTqLwNo5LKOMQf37+B1FRKRdpLVpyMxOM7PCFre7m9mpnqXKYA0NjazuMpJAjk60FpF9Q7r7CK53ztV+esM5VwNc70miDFbXFOO0bVNZOPJav6OIiLSbdIugrfmy7iK971XUkHAwdkCx31FERNpNukUw18zuMLOBZnaAmf0vMM/LYJmo4F8/4e7QnYwq6+53FBGRdpNuEVwJNAN/AR4DGoDvehUqU/XY8CYlkQSFeSG/o4iItJt0jxraBkz1OEtGS2yrpm9sNYv2+5LfUURE2lW6Rw3908y6t7hdZGYveJYqA61d8hoAkXKdSCYi+5Z0Nw2Vpo4UAsA5V02WXbO45oPXiTtj/4N1IpmI7FvSLYKEmW0fUsLMBtDGaKT7sgUNPXjKvsiAPlnVfyKSBdI9BPQnwOtm9krq9tHAJd5Eykz3bzmUA8qP4QxdsVNE9jFprRE4554HxgEfkDxy6IckjxzKCpurN7O+aiNj9i/yO4qISLtLd2fxRcBLJAvgh8CDwA1pPG6ymX1gZsvNbJdHHZnZoWYWN7OvpRe7Y61/40EWRC7i8OJ6v6OIiLS7dPcRfB84FPjYOXcsMBqo2t0DzCwA3AVMAYYB55jZsF3M9z9Axh6FFPt4NjV0ZchBO8UXEen00i2CRudcI4CZRZxz7wMH7eEx44HlzrkVzrlm4FHglDbmuxJ4EtiQZpYOV7J5PssjQ8mLZN2oGiKSBdItgsrUeQRPA/80s2fY86Uq+wIVLZ8jdd92ZtaX5GUv793dE5nZJWY218zmVlXtdkWk3UW3bqRPfA1bS0d36OuKiHSUdM8sPi315w1mNhMoBJ7fw8PaOrym9SGndwJXO+fitpujcZxz04BpAOPGjevQw1YrF75COZA/UCeSici+aa+3dTjnXtnzXEByDaCsxe1+7LwWMQ54NFUCpcCJZhZzzj29t7m8Mqe+N3+IfoPLDtGJZCKyb/qs1yxOxxxgkJmVm1kYOBt4tuUMzrly59wA59wA4Ang8kwqAYDXqvKY0eUUevco9TuKiIgnPCsC51wMuILk0UBLgcecc4vN7DIzu8yr121X8RhFK57j6L57nlVEpLPy9DAY59x0YHqr+9rcMeyc+6aXWT6LqhXvclP0l7ycVwwc73ccERFPeLlpqNP7ZHFyxNH9hh/tcxIREe+oCHYjsXo2Va6QAwcP9zuKiIhnVAS70bPmPVbmDicUDPgdRUTEMyqCXWisWU/vxDq29RzjdxQREU+pCHZhweYgE5t+hY0+1+8oIiKeUhHswrzVNVS6Hhw8eKDfUUREPKUi2IXe7/2ab3R/j5IuEb+jiIh4SkXQBhdrZnL1nzk+f7nfUUREPKciaMO6D+eRSzOBssP8jiIi4jkVQRs2LHkVgF4jvuBzEhER76kI2lI5h/WuiPIDBvudRETEcyqCNjTVVbMq/2ByAvp4RGTfpyVdK1sao5y97T+ZNeY2v6OIiHQIFUEr81fX4ByMGaDrD4hIdtDV2FsJvv4L7g+9zah+L/gdRUSkQ6gIWin55HWKwlG65oX9jiIi0iG0aaiFeLSJ/Zs+YFPRKL+jiIh0GBVBCxVLZ5NrUUIDdCKZiGQPFUELG5e+DkDfg3VFMhHJHtpH0MKSLbnU2ASO73+g31FERDqMiqCFB2pGM6j8aL5o5ncUEZEOo01DKRtravlkUzVj9y/yO4qISIdSEaRUzn6ahZGLmNhtvd9RREQ6lIogpWnlLBLkcODQ0X5HERHpUCqClO6b3mVF6EByc/P8jiIi0qFUBEBzYwPlzcuoKRnldxQRkQ6nIgBWLX6LsMUIlx/udxQRkQ6nIgDmVefzs+i5lI06zu8oIiIdTkUAvL4+zPNdv0bPXmV+RxER6XBZXwTOOXJXvsDRfXUSmYhkp6wvgvWVH/HL2K2cGnzT7ygiIr7I+iKoXPAKACVDjvI5iYiIP7K+CKIfz6bBhdl/uIaeFpHslPVFULx5PqsigwmFI35HERHxhadFYGaTzewDM1tuZlPbmH6umS1I/bxpZiO9zNNafX0d5dHlbCnVsBIikr08KwIzCwB3AVOAYcA5Zjas1WwrgS845w4BfgZM8ypPWxasa2RS823Exn67I19WRCSjeLlGMB5Y7pxb4ZxrBh4FTmk5g3PuTedcdermLKCfh3l2Mm91Datcb4YNGd6RLysiklG8LIK+QEWL25Wp+3bl28A/2ppgZpeY2Vwzm1tVVdVuAbsv/D0Xdn+XooJwuz2niEhn42URtHWGlmtzRrNjSRbB1W1Nd85Nc86Nc86N69GjR7uEc4kEJ2x+mJMj89vl+UREOisvi6ASaDlmQz9gbeuZzOwQ4H7gFOfcJg/z7GD1yg/pSTWJvod21EuKiGQkL4tgDjDIzMrNLAycDTzbcgYz6w88BZzvnPvQwyw7WbcoeSJZz2E6kUxEsptnF693zsXM7ArgBSAAPOCcW2xml6Wm3wtcB5QAd1vygvEx59w4rzK1FF/9Ng1E6DdEawQikt08KwIA59x0YHqr++5t8fdFwEVeZtiVeO1aVkWGMDQY8uPlRUQyhqdFkKlqG6JcUHcFVx1fzlC/w4jITqLRKJWVlTQ2NvodpdPJzc2lX79+hELpf8nNyiJ4d3Xy1IXR5T19TiIibamsrKRr164MGDCA1GZjSYNzjk2bNlFZWUl5eXnaj8vKsYZis+/nd6HbGdkn3+8oItKGxsZGSkpKVAJ7ycwoKSnZ6zWprFwjKFz7Gn1Cn1CQryIQyVQqgc/ms3xuWbdGEI8nKG9YTFVhh45vJyKSsbKuCFYsW0yp1ULZeL+jiEiGqqmp4e677/5Mjz3xxBOpqalp30Aey7oiWL/kNQB6DT/a5yQikql2VwTxeHy3j50+fTrdu3f3IJV3sm4fwYebYoTtYA49UNcgEOkMbnxuMUvWbmnX5xzWpxvXn7zrUYenTp3KRx99xKhRozjhhBM46aSTuPHGG+nduzfz589nyZIlnHrqqVRUVNDY2Mj3v/99LrnkEgAGDBjA3LlzqaurY8qUKUycOJE333yTvn378swzz5CXl7fDaz333HPcfPPNNDc3U1JSwsMPP8x+++1HXV0dV155JXPnzsXMuP766/nqV7/K888/z49//GPi8TilpaW89NJLn/vzyLoi+EP1wQw74EjGB7LurYtImm699VYWLVrE/PnzAXj55Zd5++23WbRo0fbDMh944AGKi4tpaGjg0EMP5atf/SolJSU7PM+yZct45JFH+O1vf8uZZ57Jk08+yXnnnbfDPBMnTmTWrFmYGffffz+33XYbv/zlL/nZz35GYWEhCxcuBKC6upqqqiouvvhiXn31VcrLy9m8eXO7vN+sWhpuqK3jk821nD9hf7+jiEiadvfNvSONHz9+h2Pzf/3rX/PXv/4VgIqKCpYtW7ZTEZSXlzNq1CgAxo4dy6pVq3Z63srKSs466yzWrVtHc3Pz9td48cUXefTRR7fPV1RUxHPPPcfRRx+9fZ7i4uJ2eW9ZtY9g1byXWBj5NkflfuR3FBHpZAoKCrb//fLLL/Piiy/y1ltv8d577zF69Og2j92PRP59LfRAIEAsFttpniuvvJIrrriChQsXct99921/HufcToeCtnVfe8iqIti24i0iFqN8qPYPiMiude3ala1bt+5yem1tLUVFReTn5/P+++8za9asz/xatbW19O2bvGbXH//4x+33T5o0id/85jfbb1dXV3P44YfzyiuvsHLlSoB22zSUVUXQZcM7VAb6Eela6ncUEclgJSUlHHnkkYwYMYKrrrpqp+mTJ08mFotxyCGHcO211zJhwoTP/Fo33HADZ5xxBkcddRSlpf9eNv30pz+lurqaESNGMHLkSGbOnEmPHj2YNm0ap59+OiNHjuSss876zK/bkjnX5kXDMta4cePc3Llz9/pxTdEY224eQEWPoxl5xZ89SCYi7WXp0qUMHaohIT+rtj4/M5u3q2H+s2aNYPkHCym2rQT6H+Z3FBGRjJI1RbC5OcTdgfPoNXqy31FERDJK1hw+etSYERw15i6/Y4iIZJysWSMQEZG2qQhERLKcikBEJMupCEREWvk8w1AD3HnnndTX17djIm+pCEREWsm2Isiao4ZEpBP7/Uk73zf8VBh/MTTXw8Nn7Dx91Ndh9LmwbRM8dsGO0y78+25frvUw1Lfffju33347jz32GE1NTZx22mnceOONbNu2jTPPPJPKykri8TjXXnst69evZ+3atRx77LGUlpYyc+bMHZ77pptu4rnnnqOhoYEjjjiC++67DzNj+fLlXHbZZVRVVREIBHj88ccZOHAgt912Gw8++CA5OTlMmTKFW2+9dS8/vD1TEYiItNJ6GOoZM2awbNky3n77bZxzfOUrX+HVV1+lqqqKPn368Pe/J4ultraWwsJC7rjjDmbOnLnDkBGfuuKKK7juuusAOP/88/nb3/7GySefzLnnnsvUqVM57bTTaGxsJJFI8I9//IOnn36a2bNnk5+f325jC7WmIhCRzLe7b/Dh/N1PLyjZ4xrAnsyYMYMZM2YwenRywMq6ujqWLVvGUUcdxY9+9COuvvpqvvzlL3PUUUft8blmzpzJbbfdRn19PZs3b2b48OEcc8wxrFmzhtNOOw2A3NxcIDkU9YUXXkh+fj7QfsNOt6YiEBHZA+cc11xzDZdeeulO0+bNm8f06dO55pprmDRp0vZv+21pbGzk8ssvZ+7cuZSVlXHDDTfQ2NjIrsZ882rY6da0s1hEpJXWw1B/6Utf4oEHHqCurg6ANWvWsGHDBtauXUt+fj7nnXceP/rRj3jnnXfafPynPr3WQGlpKXV1dTzxxBMAdOvWjX79+vH0008D0NTURH19PZMmTeKBBx7YvuNZm4ZERDpIy2Gop0yZwu23387SpUs5/PDDAejSpQsPPfQQy5cv56qrriInJ4dQKMQ999wDwCWXXMKUKVPo3bv3DjuLu3fvzsUXX8zBBx/MgAEDOPTQQ7dPe/DBB7n00ku57rrrCIVCPP7440yePJn58+czbtw4wuEwJ554Irfccku7v9+sGYZaRDoPDUP9+WgYahER2SsqAhGRLKciEJGM1Nk2W2eKz/K5qQhEJOPk5uayadMmlcFecs6xadOm7echpEtHDYlIxunXrx+VlZVUVVX5HaXTyc3NpV+/fnv1GBWBiGScUChEeXm53zGyhqebhsxsspl9YGbLzWxqG9PNzH6dmr7AzMZ4mUdERHbmWRGYWQC4C5gCDAPOMbNhrWabAgxK/VwC3ONVHhERaZuXawTjgeXOuRXOuWbgUeCUVvOcAvzJJc0CuptZbw8ziYhIK17uI+gLVLS4XQkclsY8fYF1LWcys0tIrjEA1JnZB58xUymw8TM+dl+kz2NH+jz+TZ/FjvaFz2P/XU3wsgjaGjKv9bFg6cyDc24aMO1zBzKbu6tTrLORPo8d6fP4N30WO9rXPw8vNw1VAmUtbvcD1n6GeURExENeFsEcYJCZlZtZGDgbeLbVPM8CF6SOHpoA1Drn1rV+IhER8Y5nm4acczEzuwJ4AQgADzjnFpvZZanp9wLTgROB5UA9cKFXeVI+9+alfYw+jx3p8/g3fRY72qc/j043DLWIiLQvjTUkIpLlVAQiIlkua4pgT8NdZBMzKzOzmWa21MwWm9n3/c7kNzMLmNm7ZvY3v7P4zcy6m9kTZvZ+6t/I4X5n8ouZ/Ufq/8giM3vEzPZuWM9OIiuKIM3hLrJJDPihc24oMAH4bpZ/HgDfB5b6HSJD/Ap43jk3BBhJln4uZtYX+B4wzjk3guRBL2f7m8obWVEEpDfcRdZwzq1zzr2T+nsryf/off1N5R8z6wecBNzvdxa/mVk34GjgdwDOuWbnXI2vofwVBPLMLAjks4+e55QtRbCroSyynpkNAEYDs32O4qc7gf8CEj7nyAQHAFXA71Obyu43swK/Q/nBObcG+AWwmuSwN7XOuRn+pvJGthRBWkNZZBsz6wI8CfzAObfF7zx+MLMvAxucc/P8zpIhgsAY4B7n3GhgG5CV+9TMrIjkloNyoA9QYGbn+ZvKG9lSBBrKohUzC5EsgYedc0/5ncdHRwJfMbNVJDcZHmdmD/kbyVeVQKVz7tM1xCdIFkM2+iKw0jlX5ZyLAk8BR/icyRPZUgTpDHeRNczMSG4DXuqcu8PvPH5yzl3jnOvnnBtA8t/Fv5xz++S3vnQ45z4BKszsoNRdxwNLfIzkp9XABDPLT/2fOZ59dMd5VlyqclfDXfgcy09HAucDC81sfuq+HzvnpvsXSTLIlcDDqS9NK/B+6JeM5JybbWZPAO+QPNLuXfbRoSY0xISISJbLlk1DIiKyCyoCEZEspyIQEclyKgIRkSynIhARyXIqAhGPmdkxGtVUMpmKQEQky6kIRFLM7Dwze9vM5pvZfalrFNSZ2S/N7B0ze8nMeqTmHWVms8xsgZn9NTUuDWZ2oJm9aGbvpR4zMPX0XVqM8f9w6kxVzOxWM1uSep5f+PTWJcupCEQAMxsKnAUc6ZwbBcSBc4EC4B3n3BjgFeD61EP+BFztnDsEWNji/oeBu5xzI0mOS7Mudf9o4Ackr4dxAHCkmRUDpwHDU89zs5fvUWRXVAQiSccDY4E5qWE3jie5wE4Af0nN8xAw0cwKge7OuVdS9/8RONrMugJ9nXN/BXDONTrn6lPzvO2cq3TOJYD5wABgC9AI3G9mpwOfzivSoVQEIkkG/NE5Nyr1c5Bz7oY25tvdmCxtDXf+qaYWf8eBoHMuRvKiSU8CpwLP711kkfahIhBJegn4mpn1BDCzYjPbn+T/ka+l5vk68LpzrhaoNrOjUvefD7ySuqZDpZmdmnqOiJnl7+oFU9eDKEwN9vcDYFS7vyuRNGTF6KMie+KcW2JmPwVmmFkOEAW+S/LCLMPNbB5QS3I/AsA3gHtTC/qWI3SeD9xnZjelnuOM3bxsV+CZ1AXRDfiPdn5bImnR6KMiu2Fmdc65Ln7nEPGSNg2JiGQ5rRGIiGQ5rRGIiGQ5FYGISJZTEYiIZDkVgYhIllMRiIhkuf8HoEiHOjitYRwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dataset.mnist import load_mnist\n",
    "#from two_layer_net import TwoLayerNet\n",
    "\n",
    "# 데이터 읽기\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "\n",
    "# 하이퍼파라미터\n",
    "iters_num = 6000  # 반복 횟수를 적절히 설정한다. 10에폭 돌게 설정\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100   # 미니배치 크기\n",
    "learning_rate = 0.1  # 학습률\n",
    "\n",
    "train_loss_list = []  # 오차를 담을 리스트(시각화를 위해서 데이터를 저장)\n",
    "train_acc_list = []  # 훈련 데이터의 정확도를 담을 리스트(시각화를 위해서 데이터를 저장)\n",
    "test_acc_list = []  # 테스트 데이터의 정확도를 담을 리스트(시각화를 위해서 데이터를 저장)\n",
    "\n",
    "# 1에폭당 반복 수\n",
    "iter_per_epoch = max(train_size / batch_size, 1)  # 60000 / 100 = 600\n",
    "# 1 에폭당 정확도를 시각화 하기위해 필요한 코드\n",
    "\n",
    "for i in range(iters_num):   # 60000\n",
    "    # 미니배치 획득                #60000      100    \n",
    "    batch_mask = np.random.choice(train_size, batch_size)  # 0~60000 미만 숫자에서 100개의 숫자를 랜덤추출\n",
    "    x_batch = x_train[batch_mask]  # 훈련 데이터 100개\n",
    "    t_batch = t_train[batch_mask]  # 훈련 데이터 라벨 100개\n",
    "    \n",
    "    # 기울기 계산\n",
    "    #grad = network.numerical_gradient(x_batch, t_batch)\n",
    "    grad = network.gradient(x_batch, t_batch)  # 기울기 100개를 구한다\n",
    "    \n",
    "    # 매개변수 갱신\n",
    "    for key in ('W1', 'b1', 'W2', 'b2'):   # 가중치와 바이어스를 갱신\n",
    "        network.params[key] -= learning_rate * grad[key]\n",
    "    \n",
    "    # 시각화\n",
    "    # 학습 경과 기록\n",
    "    loss = network.loss(x_batch, t_batch)  \n",
    "    train_loss_list.append(loss)  # 오차를 담는\n",
    "    \n",
    "    # 1에폭당 정확도 계산\n",
    "    if i % iter_per_epoch == 0:  # 1에폭돌때 아래의 코드를 실행해라\n",
    "        train_acc = network.accuracy(x_train, t_train)  # 훈련 데이터 정확도 출력 ?개\n",
    "        test_acc = network.accuracy(x_test, t_test)     # 테스트 데이터 정확도 출력 ?개\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "        print(\"train acc, test acc | \" + str(train_acc) + \", \" + str(test_acc))\n",
    "\n",
    "# 그래프 그리기\n",
    "markers = {'train': 'o', 'test': 's'}\n",
    "x = np.arange(len(train_acc_list))\n",
    "plt.plot(x, train_acc_list, label='train acc')\n",
    "plt.plot(x, test_acc_list, label='test acc', linestyle='--')\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('C:\\\\data\\\\jupyter\\\\deep_learning\\\\2021.03.11\\\\mnist_weight.pkl', 'wb') as f:\n",
    "    pickle.dump(network.params, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문제90. 3장에서 사용한 3층 신경망 코드를 2층 신경망으로 변경하고 위의 pickle 파일을 셋팅해서 필기체를 분류할 수 있는 신경망을 만드시오"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총  100 중에서  97 개 맞추었습니다\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from common.functions import *\n",
    "from  dataset.mnist  import  load_mnist \n",
    "import pickle\n",
    "\n",
    "def init_network():\n",
    "    with  open(\"C:\\\\data\\\\jupyter\\\\deep_learning\\\\2021.03.11\\\\mnist_weight.pkl\", \"rb\")  as  f:\n",
    "        network = pickle.load(f)\n",
    "    return  network \n",
    "\n",
    "# 1. 데이터를 불러옵니다. \n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(flatten=True, normalize=True, one_hot_label=False) \n",
    "\n",
    "# 2. 가중치와 바이어스 값을 불러옵니다.\n",
    "network = init_network()\n",
    "w1, w2 = network['W1'], network['W2']\n",
    "b1, b2 = network['b1'], network['b2']\n",
    "\n",
    "# 3. 신경망을 구성합니다. \n",
    "# 0층\n",
    "x = x_train[0:100]  # 일단 10개의 필기체 데이터를 구성합니다. \n",
    "\n",
    "# 1층\n",
    "y = np.dot(x,w1) + b1\n",
    "y_hat = sigmoid(y)\n",
    "\n",
    "# 2층\n",
    "z = np.dot(y_hat, w2) + b2\n",
    "z_hat = softmax(z)\n",
    "a = np.argmax(z_hat, axis=1)     # axis =1 이 축  # 예측값\n",
    "b = t_train[0:100]   # 실제 정답 \n",
    "\n",
    "print('총 ',len(a), '중에서 ', sum(a==b),'개 맞추었습니다')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "240.75px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
